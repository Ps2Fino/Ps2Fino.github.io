<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>"There's always money in the banana stand!"</title><link href="https://ps2fino.github.io/" rel="alternate"></link><link href="https://ps2fino.github.io/feeds/all.atom.xml" rel="self"></link><id>https://ps2fino.github.io/</id><updated>2023-12-21T11:26:03+01:00</updated><entry><title>New paper accepted in Experimental Brain Research!</title><link href="https://ps2fino.github.io/vr_motion_sickness.html" rel="alternate"></link><published>2023-12-01T00:00:00+01:00</published><updated>2023-12-01T11:48:40+01:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2023-12-01:/vr_motion_sickness.html</id><summary type="html">&lt;p&gt;We've had a paper accepted in &lt;i&gt;Experimental Brain Research&lt;/i&gt;! Here's the abstract:&lt;/p&gt;
&lt;p&gt;Virtual reality (VR) technology has been widely adopted for several professional and recreational applications. Despite rapid innovation in hardware and software, one of the long prevailing issues for end users of VR is the experience of VR sickness …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We've had a paper accepted in &lt;i&gt;Experimental Brain Research&lt;/i&gt;! Here's the abstract:&lt;/p&gt;
&lt;p&gt;Virtual reality (VR) technology has been widely adopted for several professional and recreational applications. Despite rapid innovation in hardware and software, one of the long prevailing issues for end users of VR is the experience of VR sickness. 
Females experience stronger VR sickness compared to males, and previous research has linked susceptibility to VR sickness to the menstrual cycle (Munafo, Diedrick, &amp;amp; Stoffregen, 2017). 
Here we investigated the female versus male experience in VR sickness while playing an immersive VR game, comparing days of the menstrual cycle when hormones peak: day 15 (ovulation - peak oestrogen) and day 22 (mid-luteal phase - peak progesterone).
We found that immersion duration was greater in the second session than the first, and discomfort was lessened, suggesting a powerful adaptation with repeated exposure.
Due to the oestrogen levels changing along with the exposure, there was no clear independent impact of that; note, though, that there was a significant difference between self-report and physiological measures implying that GSR is potentially an unreliable measure of motion sickness.
Although prior work found a delay over two days between session would not allow adaptation and habituation to reduce VR sickness susceptibility, we found that a week delay has potential success.&lt;/p&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;Bannigan, G. M., de Sousa, A., Scheller, M., &lt;b&gt;Finnegan, D. J.&lt;/b&gt;, Proulx, M. J.&lt;br&gt;
&lt;em&gt;Potential factors contributing to observed sex differences in Virtual Reality induced sickness&lt;/em&gt;&lt;br&gt;
Experimental Brain Research.&lt;br&gt;
DOI: forthcoming&lt;/p&gt;</content><category term="Papers"></category><category term="vr"></category><category term="virtual reality"></category><category term="motion sickness"></category><category term="menstrual cycle"></category><category term="psychophysiology"></category></entry><entry><title>New Publication in Gerontology and Geriatric Medicine!</title><link href="https://ps2fino.github.io/virtual-reality-tackling-loneliness.html" rel="alternate"></link><published>2023-06-20T00:00:00+02:00</published><updated>2023-12-21T11:26:03+01:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2023-06-20:/virtual-reality-tackling-loneliness.html</id><summary type="html">&lt;p&gt;We've had a paper accepted in &lt;i&gt;Gerontology and Geriatric Medicine&lt;/i&gt;! Here's the abstract:&lt;/p&gt;
&lt;p&gt;Current trends in gerontology conceptualize Virtual Reality (VR) as a tool for rehabilitation, lauding its potential for cognitive rehabilitation or as an intervention to reduce cognitive function decline (D'Cunha, et al., 2019) (Bauer &amp;amp; Andringa, 2020) (Sayma, Tuijt …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We've had a paper accepted in &lt;i&gt;Gerontology and Geriatric Medicine&lt;/i&gt;! Here's the abstract:&lt;/p&gt;
&lt;p&gt;Current trends in gerontology conceptualize Virtual Reality (VR) as a tool for rehabilitation, lauding its potential for cognitive rehabilitation or as an intervention to reduce cognitive function decline (D'Cunha, et al., 2019) (Bauer &amp;amp; Andringa, 2020) (Sayma, Tuijt, Cooper, &amp;amp; Walters, 2020) (Moyle, Jones, Dwan, &amp;amp; Petrovich, 2018). 
However, we must take a critical stance and identify not just the potential positive impact, but also how things may go wrong without appropriate guidelines, and the need for careful design around the interaction affordances of the technology.
We conducted co-discovery and co-design workshops involving expert stakeholders and older adults (N=20) over a period of 6 month, involving practical activities including user personas and focus groups to understand the complexities of loneliness and identify possible solutions with VR.
Based on our findings we focus our argument on two key factors in the conceptualization of loneliness: spaces, and activities which may take place within said spaces. 
We present our reconceptualization of VR as a tool for group activities instead of passive consumption of content and make a first step suggestions to the community for reducing feelings of loneliness with VR.&lt;/p&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;b&gt;Finnegan, D. J.,&lt;/b&gt; Campbell, S.&lt;br&gt;
&lt;em&gt;Tackling Loneliness and Isolation in older adults with Virtual Reality: How do we move forward?&lt;/em&gt;&lt;br&gt;
Gerontology and Geriatric Medicine.&lt;br&gt;
&lt;a href="https://dx.doi.org/10.1177/23337214231186204"&gt;DOI: 10.1177/23337214231186204&lt;/a&gt;&lt;br&gt;
&lt;a href="https://journals.sagepub.com/doi/epdf/10.1177/23337214231186204"&gt;PDF&lt;/a&gt;&lt;/p&gt;</content><category term="Papers"></category><category term="paper"></category><category term="gerontology"></category><category term="journal"></category><category term="ethics"></category><category term="virtual reality"></category><category term="mixed reality"></category><category term="augmented reality"></category><category term="perspective"></category><category term="loneliness"></category></entry><entry><title>Lancomarker: Computer Assisted Marking Administration</title><link href="https://ps2fino.github.io/lancomarker.html" rel="alternate"></link><published>2022-10-24T00:00:00+02:00</published><updated>2023-11-16T14:57:44+01:00</updated><author><name>"Daniel J. Finnegan"</name></author><id>tag:ps2fino.github.io,2022-10-24:/lancomarker.html</id><summary type="html">&lt;p&gt;Marking.
I dread it.
Not regarding the content: I believe students--when giving the right conditions to express themselves and learn--can produce fantastic scholarship.
I'm always interested in ways to improve my coursework to ensure it is making use of the current best practice in teaching and learning.
However: the burden …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Marking.
I dread it.
Not regarding the content: I believe students--when giving the right conditions to express themselves and learn--can produce fantastic scholarship.
I'm always interested in ways to improve my coursework to ensure it is making use of the current best practice in teaching and learning.
However: the burden around the &lt;em&gt;procedure&lt;/em&gt; of marking is something that is most unwelcome.
Every year I waste time having to manage hundreds of files, not counting appendices and miscellaneous image files students may include with their coursework submission.
So I built this.
You can jump straight to the video demo below by clicking &lt;a href="#video-demo"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;What is Lancomarker?&lt;/h2&gt;
&lt;p&gt;Lancomarker is a suite of web applications for automating much of the student coursework marking process.
It consists of 3 main tools: Lancomarker, Lancomarker Processing, and &lt;a target="_blank" href="https://users.cs.cf.ac.uk/FinneganD"&gt;&lt;i&gt;Lancomarker Student Interface Application (SIA)&lt;/i&gt;&lt;/a&gt;.
Together, these tools form a pipeline for information flow; students write their coursework, submit via the familar channels, and then the instructor can mark submissions and generate feedback with less administrative effort.&lt;/p&gt;
&lt;h2&gt;Why did I build Lancomarker?&lt;/h2&gt;
&lt;p&gt;In 2021, I spent 6 weeks marking student coursework.
This was intense, and meant I had no capacity for engaging in any other academic commitment during this time.
Reflecting on the workload, I realized there were severe bottlenecks throughout the marking process.
Namely, &lt;/p&gt;
&lt;p&gt;1) Student submissions: My coursework consists of essay/prose, which although may appear unusual for a computing course, is essential to assessing students' engagement with the course material as it encourages critical thinking and communication. Both of these skills are necessary for software engineers to work in teams of people with broad skillsets.
To facilitate this, students submit their coursework in PDF formats, however PDF is a terrible format for several reasons. 
First, it is immutable (for the most part): one may annotate a PDF file, but these annotations are not easily updated/modified. Secondly, it is not machine readable: it is non-trivial to parse PDF annotations and/or convert them to alternative formats. 
This is useful for transcribing comments for visually impaired students and/or collating comments into a report for feeding back to the school.
To combat this, some may suggest a WYSIWYG editor for example Microsoft Word. 
However, these all come with their own file format problems, for example incompatability across operating systems and/or how images and text are rendered. 
In the past, I have tried giving students Word templates to use but this has never been a satisfactory solution to the problem.&lt;/p&gt;
&lt;p&gt;All of these factors contributed to unnecessary administrative burden when marking as I would open and close several different applications simply to read a student's submission. 
This is related to the next point.&lt;/p&gt;
&lt;p&gt;2) Indexing student coursework: Even with relaxed criteria regarding naming and file type, finding student submissions can be a nightmare. 
I have devised the following (very rough) model for marking a coursework submission:&lt;/p&gt;
&lt;div class="math"&gt;$$ 
T_M = T_f + T_o + T_v + T_m
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(T_f\)&lt;/span&gt; is the time it takes to &lt;em&gt;find&lt;/em&gt; a student's submission in the VLE/my local file system, &lt;span class="math"&gt;\(T_o\)&lt;/span&gt; is the time to &lt;em&gt;open&lt;/em&gt; the file on my computer (which itself is a product of processor speed, number of concurrent processes, memory etc), &lt;span class="math"&gt;\(T_v\)&lt;/span&gt; is the time to &lt;em&gt;verify&lt;/em&gt; a submission, which can involve starting the whole process again to find and open a different file, and/or confirming the submission is valid (e.g., follows the rules/assessment criteria).
&lt;span class="math"&gt;\(T_m\)&lt;/span&gt; is the time taken to &lt;em&gt;mark&lt;/em&gt; a piece of coursework.
Let's assume this is kept constant as per WAM (spoiler: it's often not).&lt;/p&gt;
&lt;p&gt;Below is a simulation I wrote of this model (source in appendix).
I've removed the constant &lt;span class="math"&gt;\(T_m\)&lt;/span&gt; term for visualization.
Bear in mind this is a &lt;em&gt;very optimistic&lt;/em&gt; model, so times could be a lot worse!&lt;/p&gt;
&lt;p&gt;&lt;img src="images/simulated_marking_sessions.jpg" max-width=100% height=auto&gt;&lt;/p&gt;
&lt;p&gt;At best, this is 1.8 hours of my life I'm not getting back &lt;em&gt;every time I have a marking task to complete&lt;/em&gt; because file management is time intensive.
Lancomarker aims to eliminate this time wasting completely.&lt;/p&gt;
&lt;p&gt;3) Preparing student marks for submission to school boards: In parallel to marking, I had to manually update a spreadsheet with each students' marks. This is not just another time sink, but is also an opportunity for human error as I transcribe the mark manually by hand from the PDF comments/rubric.
Having done this several times and realized it was silly, I'd had enough.&lt;/p&gt;
&lt;p&gt;So I built Lancomarker.&lt;/p&gt;
&lt;h2&gt;How does it work?&lt;/h2&gt;
&lt;p&gt;The pipeline is as follows:&lt;/p&gt;
&lt;p&gt;1) Students complete their coursework via the SIA &lt;em&gt;or&lt;/em&gt; simply writing the text file.
This is the first time gain: I teach computer science students, which means having a basic text editor is a prerequisite.
They can use any text editor they want (even create their own), and complete their coursework.
Factor over form: what matters is the content, not the presentation here.
The SIA gives several other benefits; I can dictate filenames (something that comes useful later on) as students more often than not will incorrectly name a file through accident or malice.
I can also enforce coursework constraints such as appendices (e.g., PNG/JPG only for images) and character limits for written work.&lt;/p&gt;
&lt;p&gt;2) Students submit their work using standard channels.
This makes the system very flexible, as there is no need for them to learn a new platform if they don't want to, and there is no effort on IT and/or admin to accommodate Lancomarker format submissions.
From outside Lancomarker, the existing submission procedure is as it was.&lt;/p&gt;
&lt;p&gt;3) Submissions are batch downloaded, zipped up, then pre-processed using Lancomarker Processing.
This step creates an intermediary representation of submissions.
At this point, the instructor may generate unmarked PDFs of all submissions to mark the old way should they choose to.
This might sound odd, but it is critical to team work: my modules are (fortunately) supported by some TAs who may assist with marking, and who may not wish to use Lancomarker, instead preferring to read PDFs.
The point is this: as all submissions have been processed, they are now &lt;em&gt;readily indexable&lt;/em&gt; meaning they can be found based on a known ID (e.g., student number) rather than whatever file name was provided by the student.
Should the instructor wish to continue with Lancomarker, they can then process submissions further into the Lancomarker format.&lt;/p&gt;
&lt;p&gt;4) Submissions are then uploaded to Lancomarker.
Using Lancomarker, I can now pull submissions directly from a database, giving me direct access to a submission with a single click.
Lancomarker presents the submission as rendered HTML, and provides a UI for associating a mark value to each section of the written report.
This also facilitates error checking: as Lancomarker knows the maximum marks for a section, the instructor can't accidentally assign a mark greater than the limit as may happen when simply marking a PDF.
The UI also facilitates freetext comments for sections.&lt;/p&gt;
&lt;p&gt;5) Marked submissions are downloaded from Lancomarker, then processed further using Lancomarker Processing.
Here marked PDFs are generated, along with any CSV spreadsheets required.
As everything is machine readable, it is trivial to import marked submissions into whatever statistics software tickles your fancy for generating summary reports etc.&lt;/p&gt;
&lt;p&gt;All in all, software should &lt;em&gt;help&lt;/em&gt;, not &lt;em&gt;hinder&lt;/em&gt; work.
Marking coursework is a widely mandated activity for academics, so I was surprised that few were looking to improve the processes around marking, at least within my immediate network.
Lancomarker is deployed in the 2022/23 academic year.
Let's see if my theoretical time gain holds up in practice.&lt;/p&gt;
&lt;h2&gt;&lt;a id="video-demo"&gt;&lt;/a&gt; Video Demo&lt;/h2&gt;
&lt;p align="center"&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/K-xx8vfYA0A" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;h3&gt;Appendix&lt;/h3&gt;
&lt;p&gt;R code for those who care:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;## consider a normal distribution for opening a file&lt;/span&gt;
&lt;span class="c1"&gt;## consider a poisson distribution for finding a file&lt;/span&gt;
&lt;span class="c1"&gt;## consider a bernoulli distribution for verifying a file&lt;/span&gt;

&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;232&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;## Number of students enrolled on CM1301 in the 2022/23 Academic Year&lt;/span&gt;

&lt;span class="n"&gt;simulate_marking&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_submissions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;num_faulty_submissions&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;num_submissions&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;rbinom&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                                                      &lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_submissions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                                                      &lt;/span&gt;&lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;dd&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;tibble&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;rpois&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_submissions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;lambda&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;round&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rnormTrunc&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_submissions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                                          &lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                                          &lt;/span&gt;&lt;span class="n"&gt;sd&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                                          &lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                              &lt;/span&gt;&lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;time_to_find_open&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;dd&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;mutate&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;time_taken&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;time_to_verify_faulty&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;tibble&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;rpois&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_faulty_submissions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;lambda&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rnormTrunc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_faulty_submissions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                                    &lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                                    &lt;/span&gt;&lt;span class="n"&gt;sd&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                         &lt;/span&gt;&lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;mutate&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;time_taken&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;time_to_verify_non_faulty&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_submissions&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;num_faulty_submissions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;time_to_mark&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;## constant so ignore for now&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;total_time&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;time_to_find_open&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;time_taken&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;time_to_verify_faulty&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;time_taken&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;time_to_verify_non_faulty&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;time_to_mark&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total_time&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;## Gimme hours&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;## Run the simulation&lt;/span&gt;
&lt;span class="n"&gt;simulation_data&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;sapply&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rep&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;simulate_marking&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Technology"></category><category term="javascript"></category><category term="teaching"></category><category term="innovation"></category><category term="development"></category></entry><entry><title>Grant Success: UKRI Healthy Ageing Catalyst Award 2020!</title><link href="https://ps2fino.github.io/ha-catalyst-2020.html" rel="alternate"></link><published>2021-09-01T00:00:00+02:00</published><updated>2023-11-16T14:57:44+01:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2021-09-01:/ha-catalyst-2020.html</id><summary type="html">&lt;p&gt;I'm pleased to announce I'm a recipient of a UKRI healthy ageing catalyst award!
The grant was announced at the &lt;a href="https://healthylongevitychallenge.org/winners/exploring-how-to-use-mixed-reality-and-telepresence-to-tackle-lonliness-and-reduce-feelings-of-social-isolation/"&gt;US National Academy of Medicine&lt;/a&gt; Global Grand Challenges conference in October 2020.
Below is the abstract for my award.&lt;/p&gt;
&lt;p&gt;In the UK, it is estimated that around 1.5 million …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I'm pleased to announce I'm a recipient of a UKRI healthy ageing catalyst award!
The grant was announced at the &lt;a href="https://healthylongevitychallenge.org/winners/exploring-how-to-use-mixed-reality-and-telepresence-to-tackle-lonliness-and-reduce-feelings-of-social-isolation/"&gt;US National Academy of Medicine&lt;/a&gt; Global Grand Challenges conference in October 2020.
Below is the abstract for my award.&lt;/p&gt;
&lt;p&gt;In the UK, it is estimated that around 1.5 million aged 50 and over suffer from chronic loneliness, and 40% of older people say the television is their only company. 
These numbers are continually rising, putting a strain on social services such as the NHS, as well as other relatives who want the best for their loved ones. How is loneliness still a problem today, in a world dominated by 'social' media, where we have so many tools and technologies to connect with one another? 
The answer may lie in the shallow experience of connectedness these technologies provide. 
Connectedness is more than just being connected. 
It requires a shared understanding, the chance to socialize, agency and independence, and to share meaningful experiences and interact with other human beings. 
Through co-design workshops, low-fi prototypes, and user studies, I will explore how mixed reality—technology which lets us combine the real and digital worlds—may help establish deep relationships to relieve loneliness. 
We can chat and gossip with friends and family as if they are in the same room with us, and even host events from our own living room. 
The benefits greatly outweigh the costs: for example, the devices necessary to fulfill my vision of tackling loneliness by connecting people through mixed reality would cost less than a branded refrigerator. 
As a researcher and co-director of a community interest company, I bring a unique set of skills and intrinsic motivation to work on projects with tangible benefits for ordinary people.&lt;/p&gt;
&lt;p&gt;Please get in touch if you'd like to chat about loneliness, isolation, mixed reality, or all of the above.&lt;/p&gt;</content><category term="Awards"></category><category term="grant"></category><category term="award"></category><category term="loneliness"></category><category term="isolation"></category><category term="older adults"></category><category term="mixed reality"></category><category term="augmented reality"></category><category term="robotics"></category></entry><entry><title>New Paper in ACM TAP!</title><link href="https://ps2fino.github.io/tap-aug-2021.html" rel="alternate"></link><published>2021-08-24T00:00:00+02:00</published><updated>2023-11-16T14:57:44+01:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2021-08-24:/tap-aug-2021.html</id><summary type="html">&lt;p&gt;We've had a paper accepted into TAP!
Here's the abstract:&lt;/p&gt;
&lt;p&gt;Ultrasonic mid-air haptic technologies, which provide haptic feedback through airwaves produced using ultrasound, could be
employed to investigate the sense of body ownership and immersion in virtual reality (VR) by inducing the virtual hand illusion (VHI).
Ultrasonic mid-air haptic perception …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We've had a paper accepted into TAP!
Here's the abstract:&lt;/p&gt;
&lt;p&gt;Ultrasonic mid-air haptic technologies, which provide haptic feedback through airwaves produced using ultrasound, could be
employed to investigate the sense of body ownership and immersion in virtual reality (VR) by inducing the virtual hand illusion (VHI).
Ultrasonic mid-air haptic perception has solely been investigated for glabrous (hairless) skin, which has higher tactile sensitivity than hairy skin.
In contrast, the VHI paradigm typically targets hairy skin without comparisons to glabrous skin.
The aim of this paper was to investigate illusory body ownership, the applicability of ultrasonic mid-air haptics, and perceived immersion in VR using the VHI.
Fifty participants viewed a virtual hand being stroked by a feather synchronously and asynchronously with the ultrasonic stimulation applied to the glabrous skin on the palmar surface and the hairy skin on the dorsal surface of their hands.
Questionnaire responses revealed that synchronous stimulation induced a stronger VHI than asynchronous stimulation.
In synchronous conditions, the VHI was stronger for palmar stimulation than dorsal stimulation.
The ultrasonic stimulation was also perceived as more intense on the palmar surface compared to the dorsal surface.
Perceived immersion was not related to illusory body ownership per se but was enhanced by the provision of synchronous stimulation.&lt;/p&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;Salagean, A., Hadnett-Hunter, J. E., &lt;b&gt;Finnegan, D. J.,&lt;/b&gt; de Sousa, A., Proulx, M.&lt;br&gt;
&lt;em&gt;A Virtual Reality Application of the Rubber Hand Illusion Induced by Ultrasonic Mid-Air Haptic Stimulation&lt;/em&gt;&lt;br&gt;
ACM Transactions on Applied Perception&lt;br&gt;
&lt;a href="https://orca.cardiff.ac.uk/143639"&gt;PDF&lt;/a&gt;&lt;br&gt;
&lt;a href="https://dx.doi.org/10.1145/3487563"&gt;DOI: 10.1145/3487563&lt;/a&gt;&lt;/p&gt;</content><category term="Papers"></category><category term="paper"></category><category term="acm"></category><category term="journal"></category><category term="rubber hand illusion"></category><category term="ultraleap"></category><category term="ultrahaptics"></category><category term="perception"></category></entry><entry><title>New Publication in Frontiers in Virtual Reality!</title><link href="https://ps2fino.github.io/cassandra-complex.html" rel="alternate"></link><published>2021-05-05T00:00:00+02:00</published><updated>2023-11-16T12:28:53+01:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2021-05-05:/cassandra-complex.html</id><summary type="html">&lt;p&gt;We've had a paper accepted in &lt;i&gt;Frontiers in Virtual Reality&lt;/i&gt;! Here's the abstract:&lt;/p&gt;
&lt;p&gt;Recent years have seen a boom in Virtual, Augmented, and Mixed Reality technologies which have been widely adopted both by the consumer market and the research community. 
These technologies have provided researchers the ability to generate and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We've had a paper accepted in &lt;i&gt;Frontiers in Virtual Reality&lt;/i&gt;! Here's the abstract:&lt;/p&gt;
&lt;p&gt;Recent years have seen a boom in Virtual, Augmented, and Mixed Reality technologies which have been widely adopted both by the consumer market and the research community. 
These technologies have provided researchers the ability to generate and gather data in new ways, through world building and scenario creation in every environment imagined. 
Although this growing interest is exciting, there is also a mounting concern about best practises and ethical dilemmas. 
In the literature one can already find a large quantity of papers providing guidelines and raising ethical concerns. 
However, ethical pitfalls continue to be overlooked. 
In this opinion paper, prompted by the ethics developments in Artificial Intelligence (AI), another area with rapid growth and adoption which has been overwhelmed by a huge number of guidelines and is still nowhere close to universal acceptance of standards, we propose that the virtual, augmented, and mixed reality research and development areas need to come together as whole; involving government, industry and science in order to define, develop and decide guidelines and strategies before we replicate the devastating consequences such as decaying trust in technology witnessed in other areas like social media.&lt;/p&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;b&gt;Finnegan, D. J.,&lt;/b&gt; Zoumpoulaki, A., Eslambolchilar, P.&lt;br&gt;
&lt;em&gt;Does Mixed Reality Have a Cassandra Complex?&lt;/em&gt;&lt;br&gt;
Frontiers in Virtual Reality--Virtual Reality and Human Behaviour.&lt;br&gt;
&lt;a href="https://dx.doi.org/10.3389/frvir.2021.673547"&gt;DOI: 10.3389/frvir.2021.673547&lt;/a&gt;&lt;/p&gt;</content><category term="Papers"></category><category term="paper"></category><category term="frontiers"></category><category term="journal"></category><category term="ethics"></category><category term="virtual reality"></category><category term="mixed reality"></category><category term="augmented reality"></category><category term="opinion"></category></entry><entry><title>New Paper at INTERACT 2021!</title><link href="https://ps2fino.github.io/interact-aug-2021.html" rel="alternate"></link><published>2021-04-03T00:00:00+02:00</published><updated>2023-11-16T12:28:53+01:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2021-04-03:/interact-aug-2021.html</id><summary type="html">&lt;p&gt;We've had a paper accepted into INTERACT 2021!
Here's the abstract:&lt;/p&gt;
&lt;p&gt;Accessibility in sports media broadcast (SMB) remains a problem for blind spectators who wish to socialize and watch sports with friends and family.
Although popular, radio's reliance on low bandwidth speech results in an overwhelming experience for blind spectators …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We've had a paper accepted into INTERACT 2021!
Here's the abstract:&lt;/p&gt;
&lt;p&gt;Accessibility in sports media broadcast (SMB) remains a problem for blind spectators who wish to socialize and watch sports with friends and family.
Although popular, radio's reliance on low bandwidth speech results in an overwhelming experience for blind spectators.
In this paper we focused on two core issues: (i) how SMB can be augmented to convey diegetic information more effectively, and (ii) the social context in which SMB are consumed.
We chose tennis broadcasts for our investigations.
Addressing issue (i), we developed a system design and prototype to enhance the experience of watching tennis matches, focusing on blind spectators using audio descriptions and 3D audio, and evaluated our system with (n=12) in a controlled user evaluation.
Our results indicate how audio descriptions gave clear information for the tennis ball placements, 3D audio provided subtle cues for the ball direction, and radio provided desired human commentary.
For issue (ii), we conducted an online questionnaire (n=15) investigating the social context in which blind spectators consume SMB. 
Participant feedback indicated there is a demand for more accessible SMB content such that people can consume SMB by themselves and with their friends.
Participants were enthusiastic for a revised system design mixing elements from 3D audio and audio description.
We discuss our results in the context of social SMB spectatorship, concluding with insights into accessible SMB technologies.&lt;/p&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;Goncu, C., &amp;amp; &lt;b&gt;Finnegan, D. J.&lt;/b&gt;&lt;br&gt;
&lt;em&gt;Did you see that!?' Enhancing the Experience of Sports Media Broadcast for Blind People&lt;/em&gt;&lt;br&gt;
International Federation for Information Processing Technical Committee on Human Computer Interaction (IFIP TC13) Bi-annual International Conference (INTERACT)&lt;br&gt;
&lt;a href="http://dx.doi.org/10.1007/978-3-030-85623-6_24"&gt;DOI: 10.1007/978-3-030-85623-6_24&lt;/a&gt;&lt;/p&gt;</content><category term="Papers"></category><category term="paper"></category><category term="conference proceedings"></category><category term="ifip tc13"></category><category term="spectatorship"></category><category term="tennis"></category><category term="accessibility"></category></entry><entry><title>New Publication in ACM Journal of Computing and Cultural Heritage!</title><link href="https://ps2fino.github.io/agonistic-games.html" rel="alternate"></link><published>2020-11-12T00:00:00+01:00</published><updated>2023-11-16T14:57:44+01:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2020-11-12:/agonistic-games.html</id><summary type="html">&lt;p&gt;We've had a paper accepted in the ACM Journal of Computing and Cultural Heritage special issue on Culture Games!
This paper is an analysis of 2 games we made as part of the EU Horizon 2020 UNREST project.
We &lt;a href="https://ps2fino.github.io/chiplay-2018.html"&gt;previously wrote&lt;/a&gt; (descriptively) about the games, but this paper is a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We've had a paper accepted in the ACM Journal of Computing and Cultural Heritage special issue on Culture Games!
This paper is an analysis of 2 games we made as part of the EU Horizon 2020 UNREST project.
We &lt;a href="https://ps2fino.github.io/chiplay-2018.html"&gt;previously wrote&lt;/a&gt; (descriptively) about the games, but this paper is a comprehensive study on our installation at the Ruhr Museum, considering reactions from museum visitors and the like.
To the best of our knowledge, this paper is the first to discuss what we define as Agonistic Games: serious games which explore the theme of agonism.
Here's the abstract:&lt;/p&gt;
&lt;p&gt;In this paper, we propose Agonistic Games (AGs) as a serious games subcategory that can stimulate critical reflection on topics of dark heritage through multiperspectivity and unsettling play.
We first discuss the emerging topic of agonism in memory studies, and then how games can be used to support its objectives.
We then discuss the development of 2 original AGs: Endless Blitz and Umschlagplatz '43. 
We explore whether these two AGs were perceived as capable of stimulating critical reflection by collecting data from visitors to the exhibition 'Krieg. Macht. Sinn' at the Ruhr Museum in Germany where the games were installed, and from participants in an online course describing the games.
From analysing data collected, we outline four factors inhibiting the capacity of AGs to stimulate critical reflection (topic, context, design, and assumptions about games) and propose strategies for overcoming these inhibitors. 
Our findings are valuable to scholars, game researchers, and designers, strengthening the foundations for the design and development of future AGs.investigation to inform the design of effective remote-learning applications.&lt;/p&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;De Angeli, &lt;b&gt;D., Finnegan,&lt;/b&gt; D.J., Scott, L., O'Neill, E.&lt;br&gt;
&lt;em&gt;Unsettling Play: Perceptions of Agonistic Games&lt;/em&gt;.&lt;br&gt;
ACM Journal of Computing and Cultural Heritage Special Issue on Culture Games.&lt;br&gt;
&lt;a href="https://dx.doi.org/10.1145/3431925"&gt;DOI: 10.1145/3431925&lt;/a&gt;&lt;br&gt;
&lt;a href="https://orca.cardiff.ac.uk/136295/"&gt;PDF&lt;/a&gt;&lt;/p&gt;</content><category term="Papers"></category><category term="paper"></category><category term="acm"></category><category term="journal"></category><category term="games"></category><category term="culture"></category><category term="heritage"></category><category term="agonism"></category><category term="endless blitz"></category><category term="umschlagplatz '43"></category></entry><entry><title>New Publication in PeerJ Computer Science!</title><link href="https://ps2fino.github.io/immersive-vles-2020.html" rel="alternate"></link><published>2020-10-22T00:00:00+02:00</published><updated>2023-11-16T14:57:44+01:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2020-10-22:/immersive-vles-2020.html</id><summary type="html">&lt;p&gt;We've had a paper accepted in the PeerJ Open Access journal for Computer Science!
Here's the abstract:&lt;/p&gt;
&lt;p&gt;Massive Open Online Courses are a dominant force in remote-learning yet suffer from persisting problems stemming from lack of commitment and low completion rates. 
In this initial study we investigate how the use …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We've had a paper accepted in the PeerJ Open Access journal for Computer Science!
Here's the abstract:&lt;/p&gt;
&lt;p&gt;Massive Open Online Courses are a dominant force in remote-learning yet suffer from persisting problems stemming from lack of commitment and low completion rates. 
In this initial study we investigate how the use of immersive virtual environments for Power-Point based informational learning may benefit learners and mimic traditional lectures successfully. 
We examine the role of embodied agent tutors which are frequently implemented within virtual learning environments. 
We find similar performance on a bespoke knowledge test and metrics for motivation, satisfaction, and engagement by learners in both real and virtual environments, regardless of embodied agent tutor presence. 
Our results raise questions regarding the viability of using virtual environments for remote-learning paradigms, and we emphasise  the need for further investigation to inform the design of effective remote-learning applications.&lt;/p&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;Fitton, I. S., &lt;b&gt;Finnegan, D. J.,&lt;/b&gt; Proulx, M. J.&lt;br&gt;
&lt;em&gt;Immersive virtual environments and embodied agents for e-learning applications&lt;/em&gt;.&lt;br&gt;
PeerJ Open Access Computer Science&lt;br&gt;
&lt;a href="https://dx.doi.org/10.7717/peerj-cs.315"&gt;DOI: 10.7717/peerj-cs.315&lt;/a&gt;&lt;br&gt;
&lt;a href="https://orca.cardiff.ac.uk/135879/"&gt;PDF&lt;/a&gt;&lt;/p&gt;</content><category term="Papers"></category><category term="paper"></category><category term="peerj"></category><category term="journal"></category><category term="immersive virtual environments"></category><category term="elearning"></category><category term="vr"></category></entry><entry><title>Invited talk for Wales Tech Week</title><link href="https://ps2fino.github.io/wales-tech-week-2020.html" rel="alternate"></link><published>2020-07-13T00:00:00+02:00</published><updated>2023-11-16T12:28:53+01:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2020-07-13:/wales-tech-week-2020.html</id><summary type="html">&lt;p&gt;I presented my talk on Multisensory Perception in Virtual Reality for &lt;a href="https://technologyconnected.eventbank.com/org/walestechweek/events/"&gt;Wales Tech Week&lt;/a&gt; today.
Presented virtually via zoom, I discussed research on human perception in VR and how we can develop technologies that adapt and make a better fit for human machine systems.
You can catch a video recording …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I presented my talk on Multisensory Perception in Virtual Reality for &lt;a href="https://technologyconnected.eventbank.com/org/walestechweek/events/"&gt;Wales Tech Week&lt;/a&gt; today.
Presented virtually via zoom, I discussed research on human perception in VR and how we can develop technologies that adapt and make a better fit for human machine systems.
You can catch a video recording the of the presentation &lt;a href="https://youtu.be/k0eNtHcMUs8"&gt;here&lt;/a&gt;.
There were some great questions from an enthusiastic audience.
For slides, please check out my previous post from the &lt;a href="https://ps2fino.github.io/pint-of-science-may-2018.html"&gt;Pint of Science&lt;/a&gt; in 2018.&lt;/p&gt;</content><category term="Talks"></category><category term="science"></category><category term="public engagement"></category><category term="technology"></category><category term="vr"></category></entry><entry><title>New Publication @ IDC 2020!</title><link href="https://ps2fino.github.io/inclusive-pd-2020.html" rel="alternate"></link><published>2020-05-30T00:00:00+02:00</published><updated>2023-11-16T12:28:53+01:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2020-05-30:/inclusive-pd-2020.html</id><summary type="html">&lt;p&gt;We've had a paper accepted to the &lt;a href="https://sites.google.com/site/worldsmostinclusivepdproject/home"&gt;World's Most Inclusive Participatory Design Workshop&lt;/a&gt; @ the ACM Interaction Design &amp;amp; Children conference!
Here's the abstract:&lt;/p&gt;
&lt;p&gt;Game Jams are planned events in which attendees engage in practices of co-creation in an attempt to devise a game concept and prototype. 
They are designed to be …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We've had a paper accepted to the &lt;a href="https://sites.google.com/site/worldsmostinclusivepdproject/home"&gt;World's Most Inclusive Participatory Design Workshop&lt;/a&gt; @ the ACM Interaction Design &amp;amp; Children conference!
Here's the abstract:&lt;/p&gt;
&lt;p&gt;Game Jams are planned events in which attendees engage in practices of co-creation in an attempt to devise a game concept and prototype. 
They are designed to be fun, participatory, and stimulate creativity over a short intense period of time.
We report on a recent Game Jam, &lt;em&gt;Sacred Spring&lt;/em&gt;, aimed at educating children on the medical and scientific history of the Roman Baths in the city of Bath, UK.
In this paper, we describe the event and its output, with some brief discussion on what we learned from organizing and running the game jam with a group of children aged 6-9 years old.
Our aim is to discuss our Game Jam with the inclusive participatory design (PD) community, contextualizing novel game design and game play based learning strategies in the PD space, and devising ways to reach a broader audience in future workshops.&lt;/p&gt;
&lt;p&gt;My colleague &lt;a href="https://www.danieladeangeli.com/"&gt;Dr Daniela De Angeli&lt;/a&gt; is attending and representing our work.
No video this time, but I'll update this post with images from the workshop when I can.&lt;/p&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;De Angeli, &lt;b&gt;D., Finnegan,&lt;/b&gt; D. J., Scott, L.&lt;br&gt;
&lt;em&gt;Sacred Springs&lt;/em&gt;: Teaching Children Local History via a Game Jam.&lt;br&gt;
In: Constantin, A., Korte, J.,Wilson, C., Alexandru, C.A., Good, J., Sim, G., Read, J., Fails, J.A., Eriksson, E. (Eds.), Planning the World's Most Inclusive PD Project, ACM Interaction Design and Children (IDC) conference 2020.&lt;br&gt;
&lt;a href="https://ps2fino.github.io/documents/game-jam-inclusive-pd-2020.pdf"&gt;PDF&lt;/a&gt;&lt;/p&gt;</content><category term="Papers"></category><category term="paper"></category><category term="acm"></category><category term="workshop proceedings"></category><category term="participatory design"></category><category term="games"></category></entry><entry><title>Lancometer -- Interactive quizzes in the classroom</title><link href="https://ps2fino.github.io/lancometer.html" rel="alternate"></link><published>2020-01-29T00:00:00+01:00</published><updated>2023-11-16T12:28:53+01:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2020-01-29:/lancometer.html</id><summary type="html">&lt;p&gt;Recently, technology for real-time polling of information has become popular to do quick, interactive quizzes for presentations and also as an educational tool in the classroom, for example, tools like &lt;a href="https://www.mentimeter.com/"&gt;MentiMeter&lt;/a&gt; and &lt;a href="https://socrative.com/"&gt;Socrative&lt;/a&gt;.
Whilst I was intrigued (I'm always looking for ways to engage my students) to the advantages they …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Recently, technology for real-time polling of information has become popular to do quick, interactive quizzes for presentations and also as an educational tool in the classroom, for example, tools like &lt;a href="https://www.mentimeter.com/"&gt;MentiMeter&lt;/a&gt; and &lt;a href="https://socrative.com/"&gt;Socrative&lt;/a&gt;.
Whilst I was intrigued (I'm always looking for ways to engage my students) to the advantages they may offer towards the student experience and learning, I wasn't prepared to change my workflow to fit in to any specific format such technologies may prescribe: I generate HTML presentations using &lt;a href="https://bookdown.org/"&gt;bookdown&lt;/a&gt; and don't use PowerPoint for example.
I also didn't want to have to download any mobile application or worse, force my students to download anything (A UX nightmare for the classroom!)&lt;/p&gt;
&lt;p&gt;So over the 2019/20 christmas period, I developed a small web application to facilitate interactive sessions during my lectures.
Hosted on the school's &lt;a href="https://www.openstack.org/"&gt;OpenStack&lt;/a&gt; platform, all I do is simply embed the page in an &lt;code&gt;iframe&lt;/code&gt; using &lt;a href="https://yihui.org/knitr/"&gt;knitr&lt;/a&gt; and go from there.
The students just point their device (phone, laptop, whatever) to a URL or use the generated QR code and they can instantly join (no login/codes required).
My web development skills were rusty (haven't really done anything web based since late 2015) so it seemed like a good exercise and christmas was the only time I'd get to do it before the new teaching semester began.&lt;/p&gt;
&lt;p&gt;Having trialled it in the classroom, here are my initial observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Students &lt;em&gt;really&lt;/em&gt; liked it. Like &lt;strong&gt;a lot&lt;/strong&gt;!&lt;/li&gt;
&lt;li&gt;I was skeptical as I've heard the argument where its really just a digital 'show of hands'. However, it really is more than that. For example, its easier to visualize in a bar chart a show of hands for a set of options rather than go through each in turn. Also, and here's where I think the magic lies and is worth further study, I can see how the show of hands changes &lt;em&gt;over time&lt;/em&gt;. I have some ideas for how to study this in an experiment which I'll do at some point.&lt;/li&gt;
&lt;li&gt;Having my own system means I can discuss the internals of the system: this is useful when teaching a module like &lt;a href="https://data.cardiff.ac.uk/legacy/grails/module/CM1202/19A.html"&gt;CM1202&lt;/a&gt;. Students are also &lt;em&gt;really interested&lt;/em&gt; in seeing how things work (especially first years) and I could create a welcoming environment for feedback where they can voice their opinions and suggest new features. Suggestions are one thing but when I can then go and &lt;em&gt;implement&lt;/em&gt; their suggestions, and they see this in a future lecture (for example, the 'Show Answer' utility was something I was unsure about at first but implemented it after an initial pilot at which point a student also thought it would be a good idea), students &lt;em&gt;really&lt;/em&gt; appreciate it.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here's a video of Lancometer* in action.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/KwnxS3B43Rg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;* Or perhaps you were looking for &lt;a href="https://thelincolnite.co.uk/2019/01/limited-prints-a-fitting-tribute-to-dambusters/"&gt;this&lt;/a&gt; lancometer...&lt;/p&gt;</content><category term="Technology"></category><category term="python"></category><category term="development"></category><category term="teaching"></category><category term="lancometer"></category></entry><entry><title>Joining Cardiff University</title><link href="https://ps2fino.github.io/cadiff-may-2019.html" rel="alternate"></link><published>2019-05-06T00:00:00+02:00</published><updated>2023-11-16T12:28:53+01:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2019-05-06:/cadiff-may-2019.html</id><summary type="html">&lt;p&gt;This week I joined the &lt;a href="https://www.cardiff.ac.uk/computer-science"&gt;School of Computer Science &amp;amp; Informatics&lt;/a&gt; at Cardiff University/Prifysgol Caerdydd.
The past week has been a gentle introduction to the School, having completed induction training, I'm looking forward to taking part in the probationary training courses over the coming weeks.
I've picked up my staff …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This week I joined the &lt;a href="https://www.cardiff.ac.uk/computer-science"&gt;School of Computer Science &amp;amp; Informatics&lt;/a&gt; at Cardiff University/Prifysgol Caerdydd.
The past week has been a gentle introduction to the School, having completed induction training, I'm looking forward to taking part in the probationary training courses over the coming weeks.
I've picked up my staff access card, shifted over my website from Bath, which was painless thanks to &lt;a href="https://blog.getpelican.com/"&gt;pelican&lt;/a&gt; and my own custom batch and python scripts, and set up my HR account here at the university.
I've also spent the past few days browsing the university's intranet soaking up all the information I can.&lt;/p&gt;
&lt;p&gt;I'm slowly getting to know the city, and am looking forward to meeting all my new colleagues at the University.
Setting my goals for the summer, I'm excited about everything I'm going to learn and teach in my new post and meeting the new 2019/20 cohort of students at Cardiff.&lt;/p&gt;</content><category term="General Entry"></category><category term="cardiff"></category><category term="lecturer"></category><category term="teaching"></category><category term="moving"></category><category term="pelican"></category><category term="python"></category></entry><entry><title>New Publication @ CHI PLAY 2018!</title><link href="https://ps2fino.github.io/chiplay-2018.html" rel="alternate"></link><published>2018-09-03T00:00:00+02:00</published><updated>2019-05-29T00:00:00+02:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2018-09-03:/chiplay-2018.html</id><summary type="html">&lt;p&gt;We've had a publication accepted at CHI PLAY 2018!
Here's the abstract:&lt;/p&gt;
&lt;p&gt;Historical narratives of conflict typically revolve around heroes and villains or perpetrators and victims.
However, this dichotomy of events and people into good and evil greatly reduces the extent to which the past can be analysed, explained, and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We've had a publication accepted at CHI PLAY 2018!
Here's the abstract:&lt;/p&gt;
&lt;p&gt;Historical narratives of conflict typically revolve around heroes and villains or perpetrators and victims.
However, this dichotomy of events and people into good and evil greatly reduces the extent to which the past can be analysed, explained, and understood.
To truly understand the actions that lead to conflict, one must appreciate the dense network of relationships between social agents, each with their own personal motivations and ideals.
A contemporary political viewpoint capturing this multiperspectivity is that of Agonism.
Focusing on the characters and events, Agonism emphasises the socio-cultural interactions and relationships between all agents involved including bystanders and, crucially, perpetrators. 
We discuss two 'Games for a Social Change' that we have developed to promote an Agonistic view: &lt;em&gt;Endless Blitz&lt;/em&gt; and &lt;em&gt;Umschlag '43&lt;/em&gt;.
We describe the games themselves, and the framework of memory studies that informs our work.&lt;/p&gt;
&lt;p&gt;Here's a demo reel of both Endless Blitz and Umschlagplatz '43 in action!&lt;/p&gt;
&lt;!-- [![Endless Blitz](https://i.imgur.com/hr70Dge.png)](https://drive.google.com/file/d/1EODVqZ7YuxX5pWay5lDQEH2uxsR_aWei/view?usp=sharing "Endless Blitz") --&gt;

&lt;p align="center"&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/9YT1if8fZ9Y" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;De Angeli, &lt;b&gt;D., Finnegan,&lt;/b&gt; D. J., Scott, L., Bull, A., O'Neill, E.&lt;br&gt;
Agonistic Games: Multiperspective and Unsettling Games for a Social Change.&lt;br&gt;
CHI PLAY '18 Extended Abstracts.&lt;br&gt;
&lt;a href="http://dx.doi.org/10.1145/3270316.3270594"&gt;DOI: 10.1145/3270316.3270594&lt;/a&gt;&lt;br&gt;
&lt;a href="https://ps2fino.github.io/documents/chiplay-2018.pdf"&gt;PDF&lt;/a&gt;&lt;/p&gt;</content><category term="Papers"></category><category term="paper"></category><category term="acm"></category><category term="conference proceedings"></category><category term="agonism"></category><category term="games"></category><category term="chiplay"></category><category term="endless blitz"></category><category term="umschlagplatz '43"></category></entry><entry><title>New Publication @ ECCV 2018!</title><link href="https://ps2fino.github.io/eccv-2018.html" rel="alternate"></link><published>2018-07-26T00:00:00+02:00</published><updated>2019-07-23T00:00:00+02:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2018-07-26:/eccv-2018.html</id><summary type="html">&lt;p&gt;We've had a publication accepted at ECCV 2018!
Here's the abstract:&lt;/p&gt;
&lt;p&gt;This work presents a novel hand pose estimation framework via intermediate dense guidance map supervision. 
By leveraging the advantage of predicting heat maps of hand joints in detection-based methods, we propose to use dense feature maps through intermediate supervision …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We've had a publication accepted at ECCV 2018!
Here's the abstract:&lt;/p&gt;
&lt;p&gt;This work presents a novel hand pose estimation framework via intermediate dense guidance map supervision. 
By leveraging the advantage of predicting heat maps of hand joints in detection-based methods, we propose to use dense feature maps through intermediate supervision in a regression-based framework that is not limited to the resolution of the heat map. 
Our dense feature maps are delicately designed to encode the hand geometry and the spatial relation between local joint and global hand. 
The proposed framework significantly improves the state-of-the-art in both 2D and 3D on the recent benchmark datasets.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://xkunwu.github.io/"&gt;Xiaokun Wu&lt;/a&gt; wrote up an awesome post on the project on his &lt;a href="https://xkunwu.github.io/research/18HandPose/18HandPose"&gt;website&lt;/a&gt;.
I recommend reading it!
They say pictures speak a thousand words, so here's a gif Xiaokun made giving the general idea in a succint way.&lt;/p&gt;
&lt;p align="center"&gt;
    &lt;img src="https://xkunwu.github.io/projects/hand-track/test_seq.gif" alt="gif of hand tracking software" /&gt;
&lt;/p&gt;

&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;Wu, X., &lt;b&gt;Finnegan, D. J.,&lt;/b&gt; O'Neill, E., Yang, Y.&lt;br&gt;
HandMap: Robust Hand Pose Estimation via Intermediate Dense Guidance Map Supervision.&lt;br&gt;
ECCV 2018: Proceedings of the 15th European Conference on Computer Vision.&lt;br&gt;
&lt;a href="http://dx.doi.org/10.1007/978-3-030-01270-0_15"&gt;DOI: 10.1007/978-3-030-01270-0_15&lt;/a&gt;&lt;br&gt;
&lt;a href="https://ps2fino.github.io/documents/eccv-2018.pdf"&gt;PDF&lt;/a&gt;&lt;/p&gt;</content><category term="Papers"></category><category term="paper"></category><category term="ieee"></category><category term="conference proceedings"></category><category term="computer vision"></category><category term="hand tracking"></category></entry><entry><title>Invited talk at the Sherborne Science Cafe</title><link href="https://ps2fino.github.io/sherborne-science-cafe-2018.html" rel="alternate"></link><published>2018-07-26T00:00:00+02:00</published><updated>2019-07-30T00:00:00+02:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2018-07-26:/sherborne-science-cafe-2018.html</id><summary type="html">&lt;p&gt;Last night I gave a talk at &lt;a href="https://sherbornescafe.wixsite.com/sherbornescafe"&gt;The Sherborne Science Cafe&lt;/a&gt; in the beautiful town of &lt;a href="https://goo.gl/maps/VyXNB3UwwBU2"&gt;Sherborne&lt;/a&gt;.
It was the biggest audience I've spoken to so far (&amp;gt;&amp;gt; 50 people).
The talk was the same talk I gave at this years &lt;a href="https://ps2fino.github.io/pint-of-science-may-2018.html"&gt;Pint of Science&lt;/a&gt; with some tweaks from feedback after PoS …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Last night I gave a talk at &lt;a href="https://sherbornescafe.wixsite.com/sherbornescafe"&gt;The Sherborne Science Cafe&lt;/a&gt; in the beautiful town of &lt;a href="https://goo.gl/maps/VyXNB3UwwBU2"&gt;Sherborne&lt;/a&gt;.
It was the biggest audience I've spoken to so far (&amp;gt;&amp;gt; 50 people).
The talk was the same talk I gave at this years &lt;a href="https://ps2fino.github.io/pint-of-science-may-2018.html"&gt;Pint of Science&lt;/a&gt; with some tweaks from feedback after PoS itself.
There were many great questions from the audience ranging from the purpose of VR, the &lt;a href="https://www.oxfordscholarship.com/view/10.1093/oso/9780199674923.001.0001/oso-9780199674923-chapter-62"&gt;ethics of VR&lt;/a&gt; and machine intelligence more generally, and of course, 'When will we have the holodeck!?*'.&lt;/p&gt;
&lt;p&gt;The science cafe have written a report of the night and put it up on &lt;a href="https://drive.google.com/file/d/1WXt0zayox9fwx4lq-7tmgBG7p-5L4F52/view"&gt;their site&lt;/a&gt; which you can check out.
For video links, check out my post on the &lt;a href="https://ps2fino.github.io/bath_science_cafe.html"&gt;Raven Science Cafe&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;*&lt;a href="https://www.youtube.com/watch?v=QGUhFcRjgus"&gt;We already do&lt;/a&gt;&lt;/p&gt;</content><category term="Talks"></category><category term="cafe"></category><category term="science"></category><category term="public engagement"></category></entry><entry><title>Avalon board game companion application</title><link href="https://ps2fino.github.io/avalon-callout.html" rel="alternate"></link><published>2018-07-12T00:00:00+02:00</published><updated>2023-11-16T14:57:44+01:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2018-07-12:/avalon-callout.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;August 2023&lt;/strong&gt;: Unfortunately big G have now removed this from the store because I never got around to doing a 64 bit build. Goodbye old friend, I'll miss you dearly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;September 2018&lt;/strong&gt;: The latest version is now available as a &lt;a href="https://play.google.com/store/apps/details?id=com.Lancophone.AvalonCallout"&gt;&lt;em&gt;free download&lt;/em&gt;&lt;/a&gt; on the Google Play Store.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This past week …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;August 2023&lt;/strong&gt;: Unfortunately big G have now removed this from the store because I never got around to doing a 64 bit build. Goodbye old friend, I'll miss you dearly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;September 2018&lt;/strong&gt;: The latest version is now available as a &lt;a href="https://play.google.com/store/apps/details?id=com.Lancophone.AvalonCallout"&gt;&lt;em&gt;free download&lt;/em&gt;&lt;/a&gt; on the Google Play Store.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This past week during the evenings, I wrote a small android application just for fun.
It consists of a 3 screen UI for selecting the characters in the game of &lt;a href="https://boardgamegeek.com/boardgame/128882/resistance-avalon"&gt;Avalon&lt;/a&gt;.
Once selected, it generates the &lt;a href="https://youtu.be/b5iJjQJkWEQ?t=4m30s"&gt;callout&lt;/a&gt; for the game, and then begins to speak it back to the players via text-to-speech.&lt;/p&gt;
&lt;p&gt;Its very rough around the edges (to be fair, I made in a total of about 2.5 working hours over a few days) and contains no accessibility features as of yet.
However, some people may find it useful; I know I sure will!&lt;/p&gt;
&lt;p&gt;Its &lt;a href="https://en.wikipedia.org/wiki/Free_and_open-source_software"&gt;FOSS&lt;/a&gt; using the &lt;a href="https://opensource.org/licenses/BSD-3-Clause"&gt;BSD-3 license&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It is available from my &lt;a href="https://github.com/Ps2Fino/Avalon-App/"&gt;Github&lt;/a&gt;.
Its also &lt;a href="https://ps2fino.github.io/updater.html"&gt;Updater&lt;/a&gt; compatible.&lt;/p&gt;</content><category term="Technology"></category><category term="C#"></category><category term="development"></category></entry><entry><title>Updater -- A cmake template engine</title><link href="https://ps2fino.github.io/updater.html" rel="alternate"></link><published>2018-05-30T00:00:00+02:00</published><updated>2019-07-23T00:00:00+02:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2018-05-30:/updater.html</id><summary type="html">&lt;p&gt;Today I'm delighted to finally release Updater, a handy little template engine written in python for creating boilerplate project scaffolding code with one click.
Updater essentially creates a templated directory structure along with initial &lt;code&gt;CMakeLists.txt&lt;/code&gt; files.
This helps ensure that the project will always be compilable.&lt;/p&gt;
&lt;p&gt;Updater grew from …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today I'm delighted to finally release Updater, a handy little template engine written in python for creating boilerplate project scaffolding code with one click.
Updater essentially creates a templated directory structure along with initial &lt;code&gt;CMakeLists.txt&lt;/code&gt; files.
This helps ensure that the project will always be compilable.&lt;/p&gt;
&lt;p&gt;Updater grew from an observation I made working with students.
As they're still learning how to program, it can be overwhelming for them to ensure their code is well maintained and structured.
Using Updater enables them to focus on the implementation task, while giving me some assurance that I will always be able to build the project.&lt;/p&gt;
&lt;p&gt;Updater is &lt;a href="https://en.wikipedia.org/wiki/Free_and_open-source_software"&gt;FOSS&lt;/a&gt; using the &lt;a href="https://opensource.org/licenses/BSD-3-Clause"&gt;BSD-3 license&lt;/a&gt;.
It is available from my &lt;a href="https://github.com/Ps2Fino/Updater/releases/tag/v1.5.1"&gt;Github&lt;/a&gt;.&lt;/p&gt;</content><category term="Technology"></category><category term="python"></category><category term="development"></category><category term="research"></category></entry><entry><title>Invited talk @ Pint of Science</title><link href="https://ps2fino.github.io/pint-of-science-may-2018.html" rel="alternate"></link><published>2018-04-09T00:00:00+02:00</published><updated>2018-05-15T00:00:00+02:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2018-04-09:/pint-of-science-may-2018.html</id><summary type="html">&lt;p&gt;I've been invited to talk at Bath's Pint of Science next Month.
If you're in Bath and missed &lt;a class="reference external" href="https://ps2fino.github.io/bath_science_cafe.html"&gt;my previous talk at The Raven&lt;/a&gt;, then why not come and hear v2.0!?
I'll be talking about the close relationship between psychology and the technology involved in virtual reality (VR), and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've been invited to talk at Bath's Pint of Science next Month.
If you're in Bath and missed &lt;a class="reference external" href="https://ps2fino.github.io/bath_science_cafe.html"&gt;my previous talk at The Raven&lt;/a&gt;, then why not come and hear v2.0!?
I'll be talking about the close relationship between psychology and the technology involved in virtual reality (VR), and some of my latest research on perception in VR.
You can buy &lt;a class="reference external" href="https://pintofscience.co.uk/event/super-computers-and-ai-who-is-ruling-who"&gt;tickets for the event here&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="update"&gt;
&lt;h2&gt;Update&lt;/h2&gt;
&lt;p&gt;The night was a success!!
Some great questions asked from a curious audience.
The event was sold out, and I was delighted to take part this year.
I very much hope to do so again next year.&lt;/p&gt;
&lt;p&gt;Slides are available &lt;a class="reference external" href="https://drive.google.com/file/d/1LyhotLGBgOXW-NqTphK5WkWKU7WTN5Ph/view?usp=sharing"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="video-links"&gt;
&lt;h2&gt;Video links&lt;/h2&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;&lt;a class="reference external" href="https://www.youtube.com/watch?v=6JONMYxaZ_s"&gt;Change blindness&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;a class="reference external" href="https://www.youtube.com/watch?v=E_uZ6-0FsXo"&gt;Redirected Walking with Change Blindness&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;a class="reference external" href="https://www.youtube.com/watch?v=v-5u0z4zA_8"&gt;Haptic Retargeting&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;a class="reference external" href="https://drive.google.com/file/d/1efA10c9_o5ckRyXciuEEHLawdiVtfxeL/view?usp=sharing"&gt;Showreel of my work&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content><category term="Talks"></category><category term="science"></category><category term="public engagement"></category><category term="vr"></category></entry><entry><title>Invited talk @ CUBRIC</title><link href="https://ps2fino.github.io/cubric_may_2018.html" rel="alternate"></link><published>2018-04-05T00:00:00+02:00</published><updated>2018-05-25T00:00:00+02:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2018-04-05:/cubric_may_2018.html</id><summary type="html">&lt;p&gt;I've been invited to talk at Cardiff's &lt;a class="reference external" href="https://sites.cardiff.ac.uk/cubric/"&gt;CUBRIC&lt;/a&gt; on best programming practices in open science.
I'll cover the typeseting language &lt;a class="reference external" href="https://www.latex-project.org/about/"&gt;LaTeX&lt;/a&gt; and how it can be used in coordination with technology such as &lt;a class="reference external" href="https://www.sharelatex.com/"&gt;ShareLaTeX&lt;/a&gt; and &lt;a class="reference external" href="https://www.overleaf.com/"&gt;Overleaf&lt;/a&gt; for collaborative science writing.
I'll update this post closer to the time, but the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've been invited to talk at Cardiff's &lt;a class="reference external" href="https://sites.cardiff.ac.uk/cubric/"&gt;CUBRIC&lt;/a&gt; on best programming practices in open science.
I'll cover the typeseting language &lt;a class="reference external" href="https://www.latex-project.org/about/"&gt;LaTeX&lt;/a&gt; and how it can be used in coordination with technology such as &lt;a class="reference external" href="https://www.sharelatex.com/"&gt;ShareLaTeX&lt;/a&gt; and &lt;a class="reference external" href="https://www.overleaf.com/"&gt;Overleaf&lt;/a&gt; for collaborative science writing.
I'll update this post closer to the time, but the &lt;a class="reference external" href="https://sciprogramming.wordpress.com/schedule/"&gt;preliminary schedule is up for now&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="update"&gt;
&lt;h2&gt;Update&lt;/h2&gt;
&lt;p&gt;The talk was a success!
I had a great time; awesome to see so much enthusiasm for open science communication and software tools.
I hope I encouraged a few more folks to take the LaTeX plunge and start using it for their research papers.&lt;/p&gt;
&lt;p&gt;Slides are &lt;a class="reference external" href="https://ps2fino.github.io/documents/cubric-2018.pdf"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Talks"></category><category term="latex"></category><category term="science"></category></entry><entry><title>Rejected Proposals</title><link href="https://ps2fino.github.io/rejections.html" rel="alternate"></link><published>2018-03-26T00:00:00+02:00</published><updated>2020-01-08T00:00:00+01:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2018-03-26:/rejections.html</id><summary type="html">&lt;p&gt;There are enough instances of the &lt;a class="reference external" href="https://en.wiktionary.org/wiki/file-drawer_problem"&gt;file drawer problem&lt;/a&gt; in academia.
A similar notion applies to applications, where we only hear of the successes, perpetuating a hero narrative and brazenly skipping over the missed targets.
I don't want to add to that, so I've decided to collate this list of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;There are enough instances of the &lt;a class="reference external" href="https://en.wiktionary.org/wiki/file-drawer_problem"&gt;file drawer problem&lt;/a&gt; in academia.
A similar notion applies to applications, where we only hear of the successes, perpetuating a hero narrative and brazenly skipping over the missed targets.
I don't want to add to that, so I've decided to collate this list of grant/project proposals and applications that have been rejected.
Here are my rejections as an anti-CV.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;div class="first line-block"&gt;
&lt;div class="line"&gt;&lt;strong&gt;Research Council Grant (Co-I)&lt;/strong&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;em&gt;Application not shortlisted&lt;/em&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;em&gt;Approx. time of submission: June 2020&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- | Topic: *Exploring technology in helping people with arthritis keep active* --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;div class="first line-block"&gt;
&lt;div class="line"&gt;&lt;strong&gt;Mini doctoral training hub (PI)&lt;/strong&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;em&gt;Equivalent of 4 PhD students (~ £80k)&lt;/em&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;em&gt;Application not shortlisted&lt;/em&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;em&gt;Approx. time of submission: December 2019&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;div class="first line-block"&gt;
&lt;div class="line"&gt;&lt;strong&gt;XXX mock panel&lt;/strong&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;em&gt;Application rejected before shortlisting&lt;/em&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;em&gt;Approx. time of submission: December 2019&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;div class="first line-block"&gt;
&lt;div class="line"&gt;&lt;strong&gt;Lectureship at XXX University&lt;/strong&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;em&gt;Application rejected before interview stage&lt;/em&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;em&gt;Approx. time of submission: November 2018&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;div class="first line-block"&gt;
&lt;div class="line"&gt;&lt;strong&gt;XXX Journal Editorial Board&lt;/strong&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;em&gt;Bid for editor role; rejected after EoI submission&lt;/em&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;em&gt;Approx. time of submission: May 2018&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- | *Journal site:* `https://www.press-start.gla.ac.uk/index.php/press-start &lt;https://www.press-start.gla.ac.uk/index.php/press-start&gt;`_ --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;div class="first line-block"&gt;
&lt;div class="line"&gt;&lt;strong&gt;Gamified Employability and Transferable Skills Development&lt;/strong&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;em&gt;Internal TDF bid; rejected after EoI submission&lt;/em&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;em&gt;Approx. time of submission: February 2018&lt;/em&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;em&gt;Amount requested: £15,000&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content><category term="Grants"></category><category term="proposals"></category><category term="projects"></category><category term="rejections"></category><category term="rejection"></category></entry><entry><title>Techspark Feature</title><link href="https://ps2fino.github.io/techspark_march_2018.html" rel="alternate"></link><published>2018-03-15T00:00:00+01:00</published><updated>2023-11-16T12:28:53+01:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2018-03-15:/techspark_march_2018.html</id><summary type="html">&lt;p&gt;I was interviewed for Techspark based on a &lt;a class="reference external" href="https://ps2fino.github.io/bath_science_cafe.html"&gt;talk I gave in January&lt;/a&gt;.
The piece is light and while containing no references to published work (check out my &lt;a class="reference external" href="https://ps2fino.github.io/pages/resume.html"&gt;CV page&lt;/a&gt; if interested), it does give a fairly accurate portrayal of the kind of work I do.
You can &lt;a class="reference external" href="https://techspark.co/the-bath-research-centre-discovering-what-it-takes-to-create-a-believable-virtual-environment/"&gt;read the …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;I was interviewed for Techspark based on a &lt;a class="reference external" href="https://ps2fino.github.io/bath_science_cafe.html"&gt;talk I gave in January&lt;/a&gt;.
The piece is light and while containing no references to published work (check out my &lt;a class="reference external" href="https://ps2fino.github.io/pages/resume.html"&gt;CV page&lt;/a&gt; if interested), it does give a fairly accurate portrayal of the kind of work I do.
You can &lt;a class="reference external" href="https://techspark.co/the-bath-research-centre-discovering-what-it-takes-to-create-a-believable-virtual-environment/"&gt;read the piece here&lt;/a&gt;.&lt;/p&gt;
</content><category term="PR"></category><category term="article"></category><category term="vr"></category><category term="techspark"></category><category term="public engagement"></category></entry><entry><title>How to solve virtual reality's human perception problem</title><link href="https://ps2fino.github.io/the_conversation_march_2018.html" rel="alternate"></link><published>2018-03-05T00:00:00+01:00</published><updated>2023-11-16T12:28:52+01:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2018-03-05:/the_conversation_march_2018.html</id><summary type="html">&lt;p&gt;I wrote a piece for The Conversation on my work in psychophysics.
You can read the &lt;a class="reference external" href="https://theconversation.com/how-to-solve-virtual-realitys-human-perception-problem-92128"&gt;published article here&lt;/a&gt;.
Below is the original unedited draft I wrote for those interested.&lt;/p&gt;
&lt;div class="section" id="unedited-article"&gt;
&lt;h2&gt;Unedited Article&lt;/h2&gt;
&lt;p&gt;Outside of entertainment, virtual reality (VR) has seen significant uptake in more practical domains.
For example, using VR …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;I wrote a piece for The Conversation on my work in psychophysics.
You can read the &lt;a class="reference external" href="https://theconversation.com/how-to-solve-virtual-realitys-human-perception-problem-92128"&gt;published article here&lt;/a&gt;.
Below is the original unedited draft I wrote for those interested.&lt;/p&gt;
&lt;div class="section" id="unedited-article"&gt;
&lt;h2&gt;Unedited Article&lt;/h2&gt;
&lt;p&gt;Outside of entertainment, virtual reality (VR) has seen significant uptake in more practical domains.
For example, using VR to piece together parts of a car engine to test out a look and feel before the manufacturing process. Or to try on the latest fashion accessory before you buy.
&lt;a class="reference external" href="https://www.youtube.com/watch?v=7T4bdlRDOdk"&gt;Our own recent work&lt;/a&gt; at Bath has applied VR to exercise; imagine going to the gym to take part in the Tour de France and race against the world’s top cyclists.&lt;/p&gt;
&lt;p&gt;While VR has been successful, it is not without its kinks.
Designing an interactive system does not stop at the hardware and software; the human must be factored in too.
Perception is the term for how we take information from the world and build understanding from it.
Our perception of reality is what we base our decisions on, and &lt;a class="reference external" href="http://dx.doi.org/10.1109/TVCG.2017.2657138"&gt;mostly determines our sense of presence in an environment&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So how to tackle the problem of designing VR systems that really transport us to new worlds with an acceptable sense of presence?
As the scale of the problem grows, it becomes difficult to quantify the contribution each element of the experience makes to the person's sense of presence.
For example, when watching a 360 film in VR, it is difficult to determine if the on screen animations contribute more or less than the 360 audio technology deployed in the experience.
What we need is a method for studying VR in a reductionist manner; removing the clutter then adding piece by piece to observe the effect each has in turn.&lt;/p&gt;
&lt;p&gt;One theory blends together computer science and psychology: maximum likelihood estimation explains how we combine the information we receive across all our senses, integrating it together to inform our understanding of the environment.
In its simplest form, it states that we combine sensory information in an optimal fashion; each sense contributes an estimate of the environment but it is noisy.&lt;/p&gt;
&lt;p&gt;This scenario is depicted in the figure below which shows how the estimates from our eyes and ears combine to give an optimal estimate somewhere in the middle.
Note how the blue curve is slimmer than the other two showing decreased variance; the combined estimate takes the best of both worlds.
It is also positioned between the two sensory estimates, showing a compromise of the two.
Finally, note it is taller: this corresponds to a higher likelihood in its estimate.&lt;/p&gt;
&lt;img alt="Pictorial description of maximum likelihood estimation. 2 sensory signals are combined, with result being a signal of less variance, with a mean value in between the sensory signals" src="https://ps2fino.github.io/images/march-2018/mle.png" /&gt;
&lt;p&gt;This has many applications in VR. &lt;a class="reference external" href="http://dx.doi.org/10.1109/SIVE.2017.7901607"&gt;Our recent work&lt;/a&gt; has applied this to solving a problem in VR with how people estimate distances.
Imagine using a driving simulator for teaching people how to drive.
If people compress distances in VR, then using it as a learning environment would be inappropriate.&lt;/p&gt;
&lt;p&gt;Understanding how people integrate information from their senses is crucial to VR because it is not solely visual. Maximum Likelihood Estimation is a tool to model how effectively a VR system &lt;em&gt;needs&lt;/em&gt; to render its multisensory environment in order to deliver the desired experience. Understanding perception will lead to more immersive VR experiences.
It's not a matter of separating each signal from the noise; it's a matter of taking all signals with the noise to give the most likely result.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Articles"></category><category term="article"></category><category term="vr"></category></entry><entry><title>New Publication @ CHI 2018!</title><link href="https://ps2fino.github.io/chi-2018.html" rel="alternate"></link><published>2018-02-16T00:00:00+01:00</published><updated>2019-07-18T00:00:00+02:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2018-02-16:/chi-2018.html</id><summary type="html">&lt;p&gt;We've had a paper accepted at CHI 2018!
Here's the abstract:&lt;/p&gt;
&lt;p&gt;One of the big challenges today is to keep people healthy through sufficient exercise. 
In this project we developed a new method for improving exercise performance in a motivating manner using a VR exergame.
Players can work out on …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We've had a paper accepted at CHI 2018!
Here's the abstract:&lt;/p&gt;
&lt;p&gt;One of the big challenges today is to keep people healthy through sufficient exercise. 
In this project we developed a new method for improving exercise performance in a motivating manner using a VR exergame.
Players can work out on an exercycle doing high-intensity interval training while wearing a VR headset and racing in a virtual world. 
Our method is an interactive adaptation of Feedforward, a type of training used to achieve rapid improvements by creating self models showing previously unachieved performance levels. 
The game lets players compete against themselves, making it a bit harder for them so they are able to surpass their own previous performance. 
Our method helped players perform better and also have a better experience compared to racing against a virtual competitor.&lt;/p&gt;
&lt;p&gt;Here's a video of the work, and you can follow the the &lt;a href="https://www.bath.ac.uk/research-centres/reality-and-virtual-environments-augmentation-labs-reveal/"&gt;ReVEaL&lt;/a&gt; to see more about what we do.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/7T4bdlRDOdk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;Barathi, S. C., &lt;b&gt;Finnegan, D. J.,&lt;/b&gt; Farrow, M., Whaley, A., Heath, P., Buckley, J., Dowrick, P. W., Wünsche, B. C., Bilzon, J. L. J., O'Neill, E. &amp;amp; Lutteroth, C.&lt;br&gt;
Interactive Feedforward for Improving Performance and Maintaining Intrinsic Motivation in VR Exergaming.&lt;br&gt;
CHI 18': Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems.&lt;br&gt;
&lt;a href="http://dx.doi.org/10.1145/3173574.3173982"&gt;DOI: 10.1145/3173574.3173982&lt;/a&gt;&lt;br&gt;
&lt;a href="https://ps2fino.github.io/documents/chi-2018.pdf"&gt;PDF&lt;/a&gt;&lt;/p&gt;</content><category term="Papers"></category><category term="paper"></category><category term="acm"></category><category term="conference proceedings"></category><category term="exergame"></category><category term="games"></category></entry><entry><title>Bath Science Cafe @ The Raven</title><link href="https://ps2fino.github.io/bath_science_cafe.html" rel="alternate"></link><published>2018-01-09T00:00:00+01:00</published><updated>2018-03-29T00:00:00+02:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2018-01-09:/bath_science_cafe.html</id><summary type="html">&lt;p&gt;Last night I gave a talk at &lt;a class="reference external" href="https://goo.gl/maps/9rip47NfQnS2"&gt;The Raven&lt;/a&gt; in town as part of the &lt;a class="reference external" href="http://bathsciencecafe.org/2018/01/03/8th-january-2018-designing-virtual-reality-experiences-with-perception-in-mind/"&gt;Bath Science Cafe&lt;/a&gt;.
I was thrilled to be invited!!
My talk discussed how understanding perception can help to design virtual worlds that appear fluid and immersive.
I covered elements such as &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Redirected_walking"&gt;redirected walking&lt;/a&gt; and &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Foveated_rendering"&gt;foveated …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Last night I gave a talk at &lt;a class="reference external" href="https://goo.gl/maps/9rip47NfQnS2"&gt;The Raven&lt;/a&gt; in town as part of the &lt;a class="reference external" href="http://bathsciencecafe.org/2018/01/03/8th-january-2018-designing-virtual-reality-experiences-with-perception-in-mind/"&gt;Bath Science Cafe&lt;/a&gt;.
I was thrilled to be invited!!
My talk discussed how understanding perception can help to design virtual worlds that appear fluid and immersive.
I covered elements such as &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Redirected_walking"&gt;redirected walking&lt;/a&gt; and &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Foveated_rendering"&gt;foveated rendering&lt;/a&gt; emphasising how both techniques take advantage of human perception and deliver experiences that maintain quality while significantly reducing constraints around physical space and computational power.
I ended the talk with my own work on designing &lt;a class="reference external" href="http://dx.doi.org/10.1109/SIVE.2017.7901607"&gt;incongruent audiovisual environments&lt;/a&gt; to reduce issues with distance and scale perception in VR.
The talk was well received, and I hope to deliver it again someday!&lt;/p&gt;
&lt;div class="section" id="slides"&gt;
&lt;h2&gt;Slides&lt;/h2&gt;
&lt;p&gt;Slides are available &lt;a class="reference external" href="https://docs.google.com/presentation/d/1DQkUe9jdKwKPgNbAmYaOJCp1nNjq7rx06rOF134fNu4/edit?usp=sharing"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="video-links"&gt;
&lt;h2&gt;Video links&lt;/h2&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;&lt;a class="reference external" href="https://www.youtube.com/watch?v=6JONMYxaZ_s"&gt;Change blindness&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;a class="reference external" href="https://www.youtube.com/watch?v=THk92rev1VA"&gt;Unlimited Corridor&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;a class="reference external" href="https://www.youtube.com/watch?v=E_uZ6-0FsXo"&gt;Redirected Walking with Change Blindness&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;a class="reference external" href="https://www.youtube.com/watch?v=y74GjjBMets"&gt;Plausibility Illusion&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content><category term="Talks"></category><category term="cafe"></category><category term="science"></category><category term="public engagement"></category></entry><entry><title>EngD = Done!</title><link href="https://ps2fino.github.io/engd_complete.html" rel="alternate"></link><published>2017-06-13T00:00:00+02:00</published><updated>2023-11-16T12:28:53+01:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2017-06-13:/engd_complete.html</id><summary type="html">&lt;div class="section" id="update"&gt;
&lt;h2&gt;Update&lt;/h2&gt;
&lt;p&gt;A copy of my thesis is now available from my &lt;a class="reference external" href="https://ps2fino.github.io/pages/resume.html"&gt;cv page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Today I successfully defended my thesis Compensating for Distance Compression in Virtual Audiovisual Environments, passing my viva voce with minor corrections!
Once my corrections have been completed and submitted, I'll stick a PDF up here for anyone …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="update"&gt;
&lt;h2&gt;Update&lt;/h2&gt;
&lt;p&gt;A copy of my thesis is now available from my &lt;a class="reference external" href="https://ps2fino.github.io/pages/resume.html"&gt;cv page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Today I successfully defended my thesis Compensating for Distance Compression in Virtual Audiovisual Environments, passing my viva voce with minor corrections!
Once my corrections have been completed and submitted, I'll stick a PDF up here for anyone who might be interested.
The past 4 years have been a whirlwind, as I've written about &lt;a class="reference external" href="https://ps2fino.github.io/first-post.html"&gt;previously&lt;/a&gt; and I have learned so much.
Thanks go to my academic advisors, Prof. Eamonn O'Neill &amp;amp; Dr. Michael Proulx who have mentored me through the years, teaching me lessons I can't even describe in words.
Thanks also to Rob McHardy, my industrial advisor, who helped hone my software skills further from my BSc days into my EngD.&lt;/p&gt;
&lt;p&gt;Thanks also to the myriad of people I have interacted with over the years; each of you has influenced my thinking in a variety of different ways.
Its been one hell of a journey.
Celebrating the end of my EngD over the coming days, I look forward to my career.
Now that I have my license, I'm excied over future prospects and the projects I will get my teeth stuck into!&lt;/p&gt;
&lt;/div&gt;
</content><category term="General Entry"></category><category term="engd"></category><category term="viva"></category><category term="phd"></category></entry><entry><title>New paper accepted into SIVE 2017!</title><link href="https://ps2fino.github.io/sive-ieeevr-2017.html" rel="alternate"></link><published>2017-02-28T00:00:00+01:00</published><updated>2019-07-18T00:00:00+02:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2017-02-28:/sive-ieeevr-2017.html</id><summary type="html">&lt;p&gt;My paper titled "An Approach to Reducing Distance Compression in Audiovisual Virtual Environments" has been accepted into the &lt;a href="http://imi.aau.dk/~sts/SIVE17/"&gt;SIVE (Sonic Interactions in Virtual Environments)&lt;/a&gt; workshop as part of &lt;a href="http://ieeevr.org/2017/"&gt;IEEE VR 2017&lt;/a&gt;!
I will be presenting my paper and it will be published as part of the conference proceedings.
See you …&lt;/p&gt;</summary><content type="html">&lt;p&gt;My paper titled "An Approach to Reducing Distance Compression in Audiovisual Virtual Environments" has been accepted into the &lt;a href="http://imi.aau.dk/~sts/SIVE17/"&gt;SIVE (Sonic Interactions in Virtual Environments)&lt;/a&gt; workshop as part of &lt;a href="http://ieeevr.org/2017/"&gt;IEEE VR 2017&lt;/a&gt;!
I will be presenting my paper and it will be published as part of the conference proceedings.
See you in LA!&lt;/p&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;b&gt;Finnegan, D. J.,&lt;/b&gt; O'Neill, E., &amp;amp; Proulx, M. J.&lt;br&gt;
An approach to reducing distance compression in audiovisual virtual environments.&lt;br&gt;
In Sonic Interactions for Virtual Environments (SIVE), 2017 IEEE 3rd VR Workshop on (pp. 1-6). IEEE.&lt;br&gt;
&lt;a href="http://dx.doi.org/10.1109/SIVE.2017.7901607"&gt;DOI: 10.1109/SIVE.2017.7901607&lt;/a&gt;&lt;br&gt;
&lt;a href="https://ps2fino.github.io/documents/sive-2017.pdf"&gt;PDF&lt;/a&gt;&lt;/p&gt;</content><category term="Papers"></category><category term="ieee"></category><category term="VR"></category><category term="binaural"></category><category term="paper"></category><category term="conference proceedings"></category></entry><entry><title>Research, Anxiety, and Overcoming it all through Applied Psychology</title><link href="https://ps2fino.github.io/next-steps.html" rel="alternate"></link><published>2016-09-16T00:00:00+02:00</published><updated>2020-02-10T00:00:00+01:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2016-09-16:/next-steps.html</id><summary type="html">&lt;p&gt;I've started to reflect on my EngD, thinking about the &lt;a class="reference external" href="http://matt.might.net/articles/phd-school-in-pictures/"&gt;PhD Journey&lt;/a&gt; as a whole, and the people I've interacted with over the past 4 years.
I've experienced the ups and downs, the observed PhD curve and I've grown a lot.&lt;/p&gt;
&lt;p&gt;The biggest thing I've learned though, over the past …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've started to reflect on my EngD, thinking about the &lt;a class="reference external" href="http://matt.might.net/articles/phd-school-in-pictures/"&gt;PhD Journey&lt;/a&gt; as a whole, and the people I've interacted with over the past 4 years.
I've experienced the ups and downs, the observed PhD curve and I've grown a lot.&lt;/p&gt;
&lt;p&gt;The biggest thing I've learned though, over the past 4 years, is about research attitudes.
Knowing that there are &lt;a class="reference external" href="https://en.wikipedia.org/wiki/There_are_known_knowns"&gt;unknown unknowns&lt;/a&gt; and having the humility to accept that.
When I began my research, I felt overwhelmed: I tried and tried to review fields and areas where 'more research was needed' to no avail.
I became frustrated, anxious, and felt hopelessly lost in a sea of 'potential areas of interest' and began questioning my own ability.
Not from the healthy perspective; questions like &amp;quot;Am I right?&amp;quot;
Or &amp;quot;What if there is another way?&amp;quot;
These are good questions, and fall under my &amp;quot;humility&amp;quot; heading.
Instead, I was asking &amp;quot;Can I do this?&amp;quot; and &amp;quot;How do I know where to begin?&amp;quot;&lt;/p&gt;
&lt;p&gt;I recently completed an application for an RA position at &lt;a class="reference external" href="http://www.bath.ac.uk"&gt;Bath&lt;/a&gt;, and when I was writing my personal statement, I thought about my own approach to my research and realised that an interesting theory applied....&lt;/p&gt;
&lt;div class="section" id="the-big-five"&gt;
&lt;h2&gt;The Big Five&lt;/h2&gt;
&lt;p&gt;How would you describe personality?
Sure, you can to great lengths, with use of colorful adjectives and poetic prose to describe personality.
This is great if you're writing a book, or a reference for someone, but scientifically its way too much.
Its difficult to compare and discuss personalities from a scientific point of view with such a high dimensionality or &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)"&gt;degrees of freedom&lt;/a&gt;.
Rather, some clever psychologists attempted to reduce this dimensionality to a set of 5 core traits, and claimed that personality can be described by plotting across these five dimensions.
Sometimes called the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Big_Five_personality_traits"&gt;OCEAN&lt;/a&gt; model (scientists love acronyms, especially when those acronyms equate to actual words with ambigiously related semantics to what the acronym itself represents), it is widely applicable to many domains, and is useful as a measurement tool of personality.
Once you can measure something, you can cluster data into categories, and then develop around these clusters.&lt;/p&gt;
&lt;p&gt;From a HCI perspective, one of the most novel applications I've come across of the OCEAN theory is the thory applied to gamers (i.e people who play games often).
&lt;a class="reference external" href="https://twitter.com/the_darklorde"&gt;Jason Vandenburghe&lt;/a&gt; gave a talk at the 2014 &lt;a class="reference external" href="http://dl.acm.org/citation.cfm?id=2658537&amp;amp;CFID=668436578&amp;amp;CFTOKEN=99793062"&gt;CHIPlay&lt;/a&gt; conference in Toronto.
Sadly I wasn't at the talk, but I'd seen Vandenburghe's earlier &lt;a class="reference external" href="http://gdcvault.com/play/1015364/The-5-Domains-of-Play"&gt;GDC talk&lt;/a&gt; in 2012.
Vandenburghe applies OCEAN as a way of modelling player behaviour and their expectations from a game.
The gist is that, if you can identify personality traits of players, for example those who are more open to exploration versus those who are more introverted, perhaps less keen on direct competition, you can adjust the game mechanics and dynamics to suit that player.
Obviously, this is of direct interest to the games industry, and is useful for game designers to take into consideration.
In my personal statement, I applied OCEAN to a PhD candidate, explaining the sides of the spectrum I feel a good candidate would lie.
You can read my statement &lt;a class="reference external" href="https://ps2fino.github.io/documents/teaching-research-statement.pdf"&gt;here&lt;/a&gt;, but I'll elaborate on my choices here.
I've also kept them discipline agnostic; while I'm sure this can be further refined to fit specific disciplines (a model is a model though; always be weary of &lt;a class="reference external" href="http://blog.minitab.com/blog/adventures-in-statistics/the-danger-of-overfitting-regression-models"&gt;overfitting&lt;/a&gt;), I've opted to keep things general.
These are qualities of traits I feel any PhD student should possess.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="an-ocean-phd"&gt;
&lt;h2&gt;An OCEAN PhD&lt;/h2&gt;
&lt;p&gt;I guess at this point, I should probably elaborate a bit on the OCEAN dimensions.
The first, Openness relates to a person's apetite for new experiences.
Rather than sticking to what works, a good PhD candidate is someone who embraces new opportunities and experiences, open to failure and a tenacity to take risks.
A PhD is ultimately about learning how to deal with failure.
All the paper rejections, deadlines missed for various reasons, and feelings of complete inability to complete anything is part of the learning process.
My advisor said to me &amp;quot;There'll always be another deadline&amp;quot;.
It took me a while to fully appreciate the gravity of that sentence.
It wasn't just to pick me up, but to really drill home the notion of research as an endless cycle.
There will always be unknown unknowns, therefore there'll always be more research to do.
If theres always more research to do, there'll always be another conference venue or journal deadline.&lt;/p&gt;
&lt;p&gt;The second trait is Conscientousness.
This relates to ones ability to be resourceful and self sustainable.
&lt;a class="reference external" href="http://www.cs.bath.ac.uk/~pjw/"&gt;Professor Philip Willis&lt;/a&gt; once sent a departmental email around about how in the US they use the term PhD 'advisor' rather than the term used in the UK and Ireland, 'supervisor'.
This stuck with me as I really saw it as a more appropriate term.
A PhD should not require supervision; when you apply for a PhD in computer graphics, I would assume you have working knowledge of vector math and some experience with practical rendering techniques in theory and code.
The same applies for any discipline be it music, geography, history, politics.
The point is that this is already assumed: I would expect you to be capable of studying and working without supervision.
However, at time what you will need is guidance.
You may strike a mental wall in your research.
Advice in the form of guidance is more effective here, perhaps suggesting related avenues that you may have overlooked, or indirect research areas that, with a bit of creative thinking, may lead to breakthrough methods just waiting to be applied to your research.&lt;/p&gt;
&lt;p&gt;Up next is Extraversion.
A PhD student needs to be extraverted.
Gone are the days of solo PhD theses; massive texts stuffed full of secluded work done in a lab or an office over the course of 3 years.
The world has shifted, with globalisation catapulting us into the age of fast communication of ideas, and an ever expanding space of research domains opening up.
Its impossible to do good research alone.
Yes, I understand that a PhD is &lt;em&gt;your&lt;/em&gt; work, but that work is done in a context of people, other disciplines, and more and more information on a daily basis.
Some people even consider doing a PhD is &lt;a class="reference external" href="https://www.reddit.com/r/Futurology/comments/42boek/university_degrees_irrelevant_to_big_employers_is/"&gt;irrelevant&lt;/a&gt;.
My point here is that, tying in with Openness, a good PhD is someone who embraces methods and approaches from related research fields, and adopts an extraverted mind in approaching skill development.
Complex problems in your field may only be complex because the right tools either do not exist, or do exist, but are applied in other disciplines where they also are approriate.
Be adventurous!&lt;/p&gt;
&lt;p&gt;Agreeablenss is an interesting one, as a good PhD is somewhere in the middle.
You should agree with data that opposes your initial hypotheses and beliefs, as painful as it may be to swallow.
However, this does not mean you do so blindly.
Ask &lt;em&gt;why&lt;/em&gt; this data is the way it is?
What does it mean for your hypothesis?
Is it wrong, or just misguided?
How can you learn from it?
What if your hypothesis was originally based upon previous studies, a la &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Bayes%27_theorem"&gt;Bayes Theorem&lt;/a&gt;?
Following Bayes logic, new information should not alter your view so much if you already have a substantial background in the opposing direction.
The point here is to take in this new information, reflect on it, and then feed it back in to your new thinking.&lt;/p&gt;
&lt;p&gt;Finally, we have Neuroticism.
I'm not saying a good PhD is a never neurotic; you'd have to be a sociopath to not feel upset at times during the course of your PhD.
However, it is important to constantly reflect on your situation and understand that it is a bump in the road, not a road block.
Remain calm in the face of opposition.
Rather than a fit of hysteria, react to oppposition in a way that is productive.
Process it, then fire some questions back at it.
Don't take it at face value, but rather assimilate it and refine it.
You may find that what was initially the polar opposite may actually only have issue with certain aspects of your standpoint.
Be dynamic and invite opposition.
It'll make your PhD richer in the end.&lt;/p&gt;
&lt;/div&gt;
</content><category term="General Entry"></category><category term="research"></category><category term="teaching"></category></entry><entry><title>The Simpsons &amp; their Mathematical Secrets</title><link href="https://ps2fino.github.io/simpsons-maths.html" rel="alternate"></link><published>2015-02-04T00:00:00+01:00</published><updated>2018-03-29T00:00:00+02:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2015-02-04:/simpsons-maths.html</id><summary type="html">&lt;!-- Some directives defined for the Paul erdos link. This was tricky --&gt;
&lt;p&gt;I recently finished reading &lt;a class="reference external" href="http://www.simonsingh.net/Simpsons_Mathematics/"&gt;Simon Singh&lt;/a&gt;'s book titled 'The Simpsons &amp;amp; their Mathematical Secrets' and thought I'd write a review as it was a riveting literary account of the history of mathematics, as told through personal interviews with the writing team of both 'The Simpsons' &amp;amp; 'Futurama'.
The book does not …&lt;/p&gt;</summary><content type="html">&lt;!-- Some directives defined for the Paul erdos link. This was tricky --&gt;
&lt;p&gt;I recently finished reading &lt;a class="reference external" href="http://www.simonsingh.net/Simpsons_Mathematics/"&gt;Simon Singh&lt;/a&gt;'s book titled 'The Simpsons &amp;amp; their Mathematical Secrets' and thought I'd write a review as it was a riveting literary account of the history of mathematics, as told through personal interviews with the writing team of both 'The Simpsons' &amp;amp; 'Futurama'.
The book does not demand extensive knowledge nor professional ability and grasp at Mathematics; instead it actually illuminates some of the more difficult problems, translating them into understandable language.&lt;/p&gt;
&lt;div class="section" id="synopsis"&gt;
&lt;h2&gt;Synopsis&lt;/h2&gt;
&lt;p&gt;The book details how 'The Simpsons' has spent the past 3 decades sneakily including some nontrivial mathematics within its episodes; the reason seemingly being that the writers felt it was humourous and would engage the more mathematically inclined viewers.
Singh talks about how some episodes contain what is known as 'Freeze Frame' gags, where the visual joke appears for less than 5 frames, forcing interested parties to pause the video playback to view it.
However, other episodes place the maths in full view of the audience, in what appears to be random numbers or equations. However, upon further investigation it can be shown that the symbols and equations in episodes are far from meaningless combinations.
One of my personal favorites is as Singh describes an episode of 'Futurama' where Bender, a talking, drunken android protagonist sees a random binary sequency written in blood. Paying homage to that scene in Stanley Kubrick's The Shining', He then sees the reversed sequence through a mirror and is horrified!!
Only those with some knowledge of binary number representation would appreciate this joke; read the book to find out more!&lt;/p&gt;
&lt;p&gt;Singh introduces many famous figures in the world of mathematics throughout the book.
He spans centuries of history, from &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Archimedes"&gt;Archimedes&lt;/a&gt; to &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Leonhard_Euler"&gt;Euler&lt;/a&gt;, from &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss"&gt;Gauss&lt;/a&gt; to &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Sophie_Germain"&gt;Germain&lt;/a&gt;, and right up to &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Srinivasa_Ramanujan"&gt;Ramanujan&lt;/a&gt; and &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Paul_Erd%C5%91s"&gt;Erdős&lt;/a&gt; (he even throws in a reference to Bill Gates of Microsoft fame and his one and only published academic paper).
Some chapters are dedicated to individual mathematicians, using their achievements as a narrative, and introducing some of the problems they commited their lives too.
With each new mathematician, Singh demonstrates his ability for morphing mathematics from what can be (for most people I presume) a difficult thing to understand into intuitive, clear and sometimes beautiful explanations about one of the worlds truly fascinating subjects.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="engagement"&gt;
&lt;h2&gt;Engagement&lt;/h2&gt;
&lt;p&gt;The book reignited an old passion in me for mathematics. Having strayed from the discipline for quite some time (I developed a paralyzing fear of maths during my undergraduate studies at University after having loved the subject and performed reasonably well during highschool), I found Singh to phrase the theorems in the book in such a way that they weren't terrifying any longer.
Instead, the mathematics is juxtaposed with music, paintings, and other forms of art. It is clear from the outset that Singh believed mathematics deserves a place in this exclusive domain and does in fact state this explicitly though a quote from &lt;a class="reference external" href="http://web.stanford.edu/~kdevlin/"&gt;Keith Devlin&lt;/a&gt; regarding the degree to which &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Euler%27s_identity"&gt;Euler's equation&lt;/a&gt; reaches into the 'very depths of existence'.
Singh cleverly strays from deep technical derivations of some of the more complex topics within the main body of the book, opting to include them as appendices at the end.
This avoids a context switch from consuming some fascinating history to remembering group theory, fractals, and prime numbers.
This fits every reader from the casually piqued to the mathematics finatics obsessed with the underlying theory to satisfy their thirst for some proofs.
After all, this book is really more of a historical passage through the catacombs of modern mathematics as opposed to a university freshman textbook on linear algebra.
I found the book grossly engaging; I couldn't put it down and gorged upon its satisfactory ~250 pages in a weekend.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="verdict"&gt;
&lt;h2&gt;Verdict&lt;/h2&gt;
&lt;p&gt;I really enjoyed the book (as I imagine is quite obvious from my excited tone).
Not only did I enjoy reading about the people behind the shows, my view of the Simpsons as a 'dumb' tv show has been radically altered.
I have developed a new found appreciation for it (indeed, I'll be looking for some Freeze Frame gags myself over the next few months in re-runs!), and also some of the issues I've had with mathematics since my undergraduate years have been lifted.
I feel more confident and open to reading about mathematics again.
In a way this book has truly illuminated my bleak view on the subject.&lt;/p&gt;
&lt;p&gt;What more can I say other than; GO READ IT! :-)&lt;/p&gt;
&lt;/div&gt;
</content><category term="Book Reviews"></category><category term="mathematics"></category><category term="review"></category></entry><entry><title>Rendering spatial audio on a desktop: The SSR and APF libraries</title><link href="https://ps2fino.github.io/ssr.html" rel="alternate"></link><published>2015-02-01T00:00:00+01:00</published><updated>2018-03-29T00:00:00+02:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2015-02-01:/ssr.html</id><summary type="html">&lt;p&gt;I've been playing around with the &lt;a class="reference external" href="http://www.spatialaudio.net/ssr"&gt;SSR&lt;/a&gt; library in order to render
a binaural soundscape for a project I'm working on.
Whilst a great library, I struggled to get to grips with it for a few days.
The library makes heavy use of templates (a powerful paradigm of C++ but …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've been playing around with the &lt;a class="reference external" href="http://www.spatialaudio.net/ssr"&gt;SSR&lt;/a&gt; library in order to render
a binaural soundscape for a project I'm working on.
Whilst a great library, I struggled to get to grips with it for a few days.
The library makes heavy use of templates (a powerful paradigm of C++ but can be awkward to read) and is written
using some design patterns I had never encountered before.&lt;/p&gt;
&lt;p&gt;I started by reading some of the source code, trying to compile the header-only library and get some test code working.
After spending hours getting the library to compile, I was ready to try it out with some sample assets from
&lt;a class="reference external" href="https://ps2fino.github.io/first-post.html#audio-defence"&gt;Audio Defence&lt;/a&gt;. Compiler errors began to arise upon simply instantiating the ssr::BinauralRenderer so I had to dig a little deeper.
I had a read of the source code and I came across some examples that were included in the repository.
I also emailed one of the author's of the library to ask for some help with integrating the SSR as a library into my existing framework in order to handle the binaural rendering.&lt;/p&gt;
&lt;p&gt;After a week of bashing my head against the wall, I've now managed to pump out some spatial sound from the renderer, sending it out to the speakers and dumping it to a stereo file.
I've decided to jot down some notes here to help me remember how the thing works!!&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The ssr::BinauralRenderer is a subclass of the apf::MimoProcessor, which is an abstract multiple input/multiple
output processor that enables the programmer to implement the processing callback while handling the threading
and access control of the samples held in the processor's buffer.
The renderer is instantiated by passing a apf::paramter_map instance which is a key-value dictionary of configuration settings for the renderer. The main settings required are the sample rate, block size and the location (full path) to the HRIR file that contains the impulse responses for convoluion.&lt;/li&gt;
&lt;li&gt;The processor functions through use of the
&lt;a class="reference external" href="http://en.wikipedia.org/wiki/Policy-based_design"&gt;Policy Design Pattern&lt;/a&gt;. This design pattern dictates that
a can have a number of different policies for
responding to similar situations (ie. a class may have a number of different policies regarding the printing of
data to a file or to a TCP stream).
In my case, the policies that the renderer is concerned with are its interface (how to process it's data
buffers at each audio cycle or rather how to 'use' and interface with it) and how to act in a threaded manner.
To specify which policies to use, you simply include the header file of the policy and define a macro called APF_MIMOPROCESSOR_INTERFACE_POLICY for the interface policy and APF_MIMOPROCESSOR_THREAD_POLICY for the thread policy. The library comes with a default thread header which just uses a single threaded policy (ie. not implemented) on windows and the POSIX library for *nix and OSX systems.&lt;/li&gt;
&lt;li&gt;In order to use the binaural rendererer, you need to specify the policies you want to use. The renderer relies
on two policies in it's implementation; an interface policy and a threading policy. To use the renderer as a standalone module, the pointer policy must be used. This then opens up the audioCallback function to be called manually by the application programmer when they want to process some data. The function accepts 3 arguments, the block size of the frame to be processed, a pointer to a series of inputs and a pointer to a pair of outputs (as the binaural renderer is an instance of an N-input, 2-output processor for stereo binaural output).&lt;/li&gt;
&lt;li&gt;The processor requires it's input to be a pointer to a list of channels. These channels can be implemented as a
series of vectors. The renderer's output is also expected to be a series of vectors representing the audio channels. The inputs should be a N * BLOCK_SIZE matrix where BLOCK_SIZE is the number of frames to be processed as a block during each run of the audio cycle. The N is the number of channels in the input. The outputs should be a 2 * BLOCK_SIZE matrix, indicating stereo output.
The renderer expects a 1-1 mapping of input channels to sources, and the sources are ordered with the channels (ie Channels[0] is the first source, Channels[1] the second etc...)&lt;/li&gt;
&lt;li&gt;Finally, for dumping to a file, you need to transpose the channels as libsndfile reads in row-wise order,
intereleaving the channels as it dumps to the file. This was the major source of confusion for me and it took some fiddling and multiple reads of the source repo to understand how that worked.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So to recap; the binaural renderer can be instantiated after specifying the policies required to do it's thing.
Next you need to generate a parameter map, a key-value dictionary containing the configuration (block size, HRIR file path etc.) for the renderer. In order to use the renderer, you pass a pointer to a list of arrays representing the channels of the audio (best to use the apf::fixed_matrix container that comes with the APF framework).&lt;/p&gt;
&lt;p&gt;A blunder was in dumping the output to a file; this wasn't the SSR's fault as &lt;a class="reference external" href="http://www.mega-nerd.com/libsndfile/api.html"&gt;libsndfile&lt;/a&gt; expects reads and writes in row wise order for (de)interleaving. All I had to do here is have a second output buffer which is the transposed matrix of the output list of channels. You can then call the writef() function of the SndFileHandle object in the libsndfile C++ API for writing stereo output to the file.&lt;/p&gt;
&lt;p&gt;Having figured all of this out I can finally move onto the image processing aspect of my project. More on this to come later.&lt;/p&gt;
</content><category term="Technology"></category><category term="binaural"></category><category term="C++"></category><category term="development"></category><category term="research"></category></entry><entry><title>Auditory saliency and its role in accessible gaming</title><link href="https://ps2fino.github.io/saliency.html" rel="alternate"></link><published>2014-12-03T00:00:00+01:00</published><updated>2018-03-29T00:00:00+02:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2014-12-03:/saliency.html</id><summary type="html">&lt;p&gt;Today I met with my supervisors to discuss my upcoming transfer viva and interesting topics for studies moving into 2015.
While nearing completion, my supervisors gave some valuable feedback for the tone and message of my report; especially the necessity of a coherent narrative.
I've tried to be as thorough …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today I met with my supervisors to discuss my upcoming transfer viva and interesting topics for studies moving into 2015.
While nearing completion, my supervisors gave some valuable feedback for the tone and message of my report; especially the necessity of a coherent narrative.
I've tried to be as thorough as possible in my report writing but I need to make sure that the reason for reading it is conveyed clearly to the reader.&lt;/p&gt;
&lt;p&gt;After discussing the report we moved on to the interesting stuff; what to start working on next.
The overarching theme of my thesis is inclusive gaming.
By this, I am interested in design techniques and technologies that can enable games to reach a wider audience by becoming artefacts of an accessible design.&lt;/p&gt;
&lt;p&gt;How can an interface surpass the status quo and becoming &amp;quot;accessible&amp;quot; whilst maintaining the same level of efficiency as one for sighted players?
Given that our aural senses have a much smaller capacity for interpreting information when compared to our visual sense, this sounds like it is unachievable.
However, a lot of the information our visual sense perceives in the occipital lobe and propogates through the visual cortex is redundant, irrelevant.
Most of the time, there is a subset of the data perceived that is of interest or rather necessary to understand the scene which we are observing.
In other words, what we want is salience.&lt;/p&gt;
&lt;p&gt;Whilst current accessible tools focus on mapping visual elements 1-1 to an equal yet slower modality (temporality issues exist in screen readers and text-to-speech systems), might there be a way to extract the information in a visual scene, remove non-salient information and then compress it into a form that enables a ~1:1 mapping of the objects in the scene and the sounds used to represent them?&lt;/p&gt;
&lt;p&gt;Back to the books for me. Below are references shared with me by my supervisor. Passing them on here.&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;&lt;a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0960982205011103#"&gt;Auditory Salience&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;&lt;a class="reference external" href="http://link.springer.com/article/10.3758%2Fs13414-010-0073-7#page-1"&gt;Crossmodal Correspondences&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;
</content><category term="General Entry"></category><category term="saliency"></category><category term="research"></category></entry><entry><title>First Post</title><link href="https://ps2fino.github.io/first-post.html" rel="alternate"></link><published>2014-11-09T00:00:00+01:00</published><updated>2019-05-10T00:00:00+02:00</updated><author><name>Daniel J. Finnegan</name></author><id>tag:ps2fino.github.io,2014-11-09:/first-post.html</id><summary type="html">&lt;!-- Hyper links --&gt;
&lt;p&gt;So, I set up this site almost a year ago now and never actually published anything to it.
I've decided to have a bash at it and might try and publish to it at least once a week, probably on a Sunday afternoon ( much like this one :-) ).&lt;/p&gt;
&lt;p&gt;I've often tried …&lt;/p&gt;</summary><content type="html">&lt;!-- Hyper links --&gt;
&lt;p&gt;So, I set up this site almost a year ago now and never actually published anything to it.
I've decided to have a bash at it and might try and publish to it at least once a week, probably on a Sunday afternoon ( much like this one :-) ).&lt;/p&gt;
&lt;p&gt;I've often tried blogging in the past but could never quite do it.
I've found it uncomfortable, difficult or frustrating in the past to write things down in a public format as I've always kept notes in the past.
I've been thinking however that this maintaining a blog might be a good idea for my EngD as it will certainly help build up a corpus of writing which may prove useful in two years time when it comes to the viva.&lt;/p&gt;
&lt;p&gt;This past year has seen a lot happen in my academic life.
Highlights include:&lt;/p&gt;
&lt;div class="section" id="ubiss-1"&gt;
&lt;h2&gt;UBISS&lt;/h2&gt;
&lt;p&gt;Taking part in the 5th UBI Summer School in Oulu (link &lt;a class="reference external" href="http://www.ubioulu.fi/en/UBI-summer-school-2014"&gt;UBISS&lt;/a&gt;), where my team took home the best project award from Workshop D.
The workshop was given by Dr. Floyd Mueller from RMIT in Melbourne, Australia.
The workshop involved exploring the design of physical based games that incorporate digital technology in some form.
Dr. Mueller has a nicer terminology; 'Bodily Play'.
The school proved an indispensable week full of activities from learning interesting research in physical gaming as well as meeting many students from around the world studying STEM disciplines.
The best part was that the game I created with my team, Reindeer &amp;amp; Wolves, was &lt;a class="reference external" href="http://dx.doi.org/10.1145/2658537.2661309"&gt;published as a short paper&lt;/a&gt; at the CHI PLAY conference in Toronto.
There is a QR code at the bottom of this page with a DOI embedded for the article.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="audio-defence"&gt;
&lt;h2&gt;Audio Defence&lt;/h2&gt;
&lt;p&gt;Winning the accessibility award at the TIGA Games Industry Awards in London.
The ceremony took place in November and a game I have worked on the past year with my partner company, &lt;a class="reference external" href="http://www.somethinelse.com"&gt;Somethin' Else&lt;/a&gt;, won the Accessibility award.
We were delighted to have won the award in recognition of the work we've been doing over the past year.
The game is a first person shooter where the enemies (or shoot-ies) are rendered binaurally for a 3D spatial effect in real time.
The game is available on the App Store &lt;a class="reference external" href="https://itunes.apple.com/gb/app/audio-defence-zombie-arena/id804041240?mt=8"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As I approach the end of the year, I am prepping for my transfer report and viva.
This I hope will go smoothly as I want to demonstrate the work I've done so far as well as pose a structured plan for the next 2 years.&lt;/p&gt;
&lt;p&gt;I'll write my next post next week.
So for now, I'm signing off.&lt;/p&gt;
&lt;!-- This is the correct syntax for displaying an image Dan :-) --&gt;
&lt;img alt="QR code for DOI: 10.1145/2658537.2661309" src="https://ps2fino.github.io/images/qrImage.png" /&gt;
&lt;/div&gt;
</content><category term="General Entry"></category><category term="ubiss"></category><category term="CHIPlay"></category><category term="acm"></category><category term="games"></category></entry></feed>